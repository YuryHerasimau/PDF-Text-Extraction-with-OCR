{
    "pdf_path": "input/Microservices_Designing_Deploying.pdf",
    "pages": [
        {
            "page_number": 1,
            "text": "From Design to Deployment",
            "status": "success"
        },
        {
            "page_number": 2,
            "text": "MICROSERVICES\n\nFrom Design to Deployment\n\nNGiNX",
            "status": "success"
        },
        {
            "page_number": 3,
            "text": "Table of Contents\n\nForeword .. 0... . cee iii\nIntroduction to Microservices............... 00.0000.\nBuilding Monolithic Applications... 0... 0.0.0... .020004\nMarching Toward Monolithic Hell... 0.0.0.0... 0. ...02000. 3\nMicroservices — Tackling the Complexity ................ 4\nThe Benefits of Microservices ... 0... 0.0.0.0 0000000 ee 8\nThe Drawbacks of Microservices ..........0 000000000 9\nSummary. 2... ee\nicroservices in Action: NGINX Plus as a Reverse Proxy Server. .\nUsing an API Gateway... 1... 2.0.0.0... 00.0 ee eee 2\nntroduction. 2... ee 2\nDirect Client-to-Microservice Communication ........... 5\nUsing an API Gateway... 1... 0.0... ee 5\nBenefits and Drawbacks of an API Gateway.............. 7\nmplementing an API Gateway.......... 2.200.000 0004 7\nPerformance and Scalability .................000. 7\nUsing a Reactive Programming Model............... 8\nService Invocation... 2... 0. ee 8\nService Discovery... . 0... ee 9\nHandling Partial Failures... 2.0... 00.00.0002, 9\nSummary. 2... ee 20\nicroservices in Action: NGINX Plus as an API Gateway... . . 20\nInter-Process Communication..................000. 21\nntroduction. 2... ee 21\nnteraction Styles... 0... ee 22\nDefiningAPIS. 6... 2... ee 24\nEvolvingAPIS.. 0.0... ee 24\nHandling Partial Failure... ee 25\nPC Technologies... . ee 26\nAsynchronous, Message-Based Communication ......... 26\nSynchronous, Request/Response IPC ..............005 29\nREST 0. ee eee 29\nThrift. ee 31\nMessage Formats. ... 0.0.00... 0 eee eee 31\nSummary... ee 32\n\nMicroservices in Action: NGINX and Application Architecture. . .33",
            "status": "success"
        },
        {
            "page_number": 4,
            "text": "4 Service Discovery .... 0.0.0.0... 0.0 eee 34\n\nWhy Use Service Discovery? .... 0.0.0.0. 00.0000 eee 34\nThe Client-Side Discovery Pattern... ............ 0000. 35\nThe Server-Side Discovery Pattern... ........... 0000. 37\nThe Service Registry... 0.0... ee 38\nService Registration Options ............ 0.0.02... 00. 39\nThe Self-Registration Pattern... . 0... ee ee 39\nThe Third-Party Registration Pattern... .......... 0000. 4\nSUMMALY. 2 ee 42\nicroservices in Action: NGINX Flexibility... 0... 0... ..00. 43\nEvent-Driven Data Management for Microservices........ 44\nicroservices and the Problem of Distributed\nDataManagement ..... 0.2.0.0... 000 eee ee 44\nEvent-Driven Architecture... 0.0... 2. ee ee ee 47\nAchieving Atomicity ... 0... 0.2 ee ee 50\nPublishing Events Using Local Transactions ............. 50\nining a Database TransactionLog................00. 51\nUsing Event Sourcing... 2... ee 52\nSUMMALY. 2 ee 54\nicroservices in Action: NGINX and Storage Optimization. . . .54\nChoosing a Microservices Deployment Strategy......... 55\notivationS . 6. ee 55\nultiple Service Instances per Host Pattern... .......... 56\nService Instance per Host Pattern.................0.04 58\nService Instance per Virtual Machine Pattern. ........... 58\nService Instance per Container Pattern................ 60\nServerless Deployment... . 0... 00.00.0000 eee eee 62\nSUMMArY. oo ee 63\nMicroservices in Action: Deploying Microservices\nAcross Varying Hosts with NGINX... 02.0.0... 2 ee 63\nRefactoring a Monolith into Microservices.............. 64\nOverview of Refactoring to Microservices............... 65\nStrategy #1:Stop Digging... .. 0... 0.0.0.0... ee eee 66\nStrategy #2: Split Frontend and Backend ............... 67\nStrategy #3: Extract Services... 2... 00.00... e eee 69\nPrioritizing Which Modules to Convert into Services ..... 69\nHow to ExtractaModule......... 2.0... 0000000004 69\nSUMMArY. oo ee 71\nMicroservices in Action: Taming a Monolith with NGINX... .. . 72\nResources for Microservices and NGINX. .............. 73",
            "status": "success"
        },
        {
            "page_number": 5,
            "text": "Foreword\n\nby Floyd Smith\n\nThe rise of microservices has been a remarkable advancement in application\ndevelopment and deployment. With microservices, an application is developed,\n\nor refactored, into separate services that “speak” to one another in a well-defined way —\nvia APIs, for instance. Each microservice is self-contained, each maintains its own\ndata store (which has significant implications), and each can be updated independently\nof others.\n\noving to a microservices-based approach makes app development faster and easier\noO manage, requiring fewer people to implement more new features. Changes can be\nmade and deployed faster and easier. An application designed as a collection of\nmicroservices is easier to run on multiple servers with load balancing, making it easy\no handle demand spikes and steady increases in demand over time, while reducing\ndowntime caused by hardware or software problems.\n\nicroservices are a critical part of anumber of significant advancements that are\nchanging the nature of how we work. Agile software development techniques, moving\napplications to the cloud, DevOps culture, continuous integration and continuous\ndeployment (CI/CD), and the use of containers are all being used alongside microservices\n0 revolutionize application development and delivery.\n\nNGINX software is strongly associated with microservices and all of the technologies\nisted above. Whether deployed as a reverse proxy, or as a highly efficient web server,\nNGINX makes microservices-based application development easier and keeps\nmicroservices-based solutions running smoothly.\n\nWith the tie between NGINX and microservices being so strong, we've run a seven-part\nseries on microservices on the NGINX website. Written by Chris Richardson, who has\nhad early involvement with the concept and its implementation, the blog posts cover\nthe major aspects of microservices for app design and development, including how to\nmake the move from a monolithic application. The blog posts offer a thorough overview\nof major microservices issues and have been extremely popular.",
            "status": "success"
        },
        {
            "page_number": 6,
            "text": "In this ebook, we've converted each blog post to a book chapter, and added a sidebar\nto each chapter with information relevant to implementing microservices in NGINX.\nIf you follow the advice herein carefully, you'll solve many potential development\nproblems before you even start writing code. This book is also a good companion\n\nto the NGINX Microservices Reference Architecture, which implements much of the\ntheory presented here.\n\nThe book chapters are:\n\n1. Introduction to Microservices — A clear and simple introduction to microservices,\nfrom its perhaps overhyped conceptual definition to the reality of how microservices\nare deployed in creating and maintaining applications.\n\n2. Using an API Gateway — An API Gateway is the single point of entry for your entire\nmicroservices-based application, presenting the API for each microservice. NGINX Plus\ncan effectively be used as an API Gateway with load balancing, static file caching,\nand more.\n\n3. Inter-process Communication in a Microservices Architecture — Once you break\na monolithic application into separate pieces — microservices — the pieces need to\nspeak to each other. And it turns out that you have many options for inter-process\ncommunication, including representational state transfer (REST). This chapter gives\nthe details.\n\n4. Service Discovery in a Microservices Architecture — When services are running\nina dynamic environment, finding them when you need them is not a trivial issue.\nIn this chapter, Chris describes a practical solution to this problem.\n\n5. Event-Driven Data Management for Microservices — Instead of sharing a unified\napplication-wide data store (or two) across a monolithic application, each microservice\nmaintains its own unique data representation and storage. This gives you great\n\nlexibility, but can also cause complexity, and this chapter helps you sort through it.\n\n6. Choosing a Microservices Deployment Strategy — In a DevOps world, how you do\nhings is just as important as what you set out to do in the first place. Chris describes\nhe major patterns for microservices deployment so you can make an informed\nchoice for your own application.\n\n7. Refactoring a Monolith into Microservices — In a perfect world, we would always get\nhe time and money to convert core software into the latest and greatest technologies,\nools, and approaches, with no real deadlines. But you may well find yourself converting\na monolith into microservices, one... small... piece... at... a... time. Chris presents a\nstrategy for doing this sensibly.\n\nWe think you'll find every chapter worthwhile, and we hope that you'll come back to this\nebook as you develop your own microservices apps.\n\nFloyd Smith\nNGINX, Inc.",
            "status": "success"
        },
        {
            "page_number": 7,
            "text": "Introduction to\nMicroservices\n\nMicroservices are currently getting a lot of attention: articles, blogs, discussions on\nsocial media, and conference presentations. They are rapidly heading towards the peak\nof inflated expectations on the Gartner Hype cycle. At the same time, there are skeptics\nin the software community who dismiss microservices as nothing new. Naysayers claim\nthat the idea is just a rebranding of service-oriented architecture (SOA). However, despite\nboth the hype and the skepticism, the Microservices Architecture pattern has significant\nbenefits — especially when it comes to enabling the agile development and delivery of\ncomplex enterprise applications.\n\nThis chapter is the first in this seven-chapter ebook about designing, building,\n\nand deploying microservices. You will learn about the microservices approach and\nhow it compares to the more traditional Monolithic Architecture pattern. This ebook\nwill describe the various elements of a microservices architecture. You will learn about\nthe benefits and drawbacks of the Microservices Architecture pattern, whether it makes\nsense for your project, and how to apply it.\n\noO\n\n's first look at why you should consider using microservices.\n\nBuilding Monolithic Applications\n\nLet's imagine that you were starting to build a brand new taxi-hailing application\nintended to compete with Uber and Hailo. After some preliminary meetings and\nrequirements gathering, you would create a new project either manually or by using\na generator that comes with a platform such as Rails, Spring Boot, Play, or Maven.\n\noO\n\nMicroservices — From Design to Deployment 1 LT ‘oduction to",
            "status": "success"
        },
        {
            "page_number": 8,
            "text": "This new application would have a modular hexagonal architecture, like in Figure 1-1:\n\nMonolithic\nO oP mysan. | Architecture\n\nPASSENGER,\n\n:\n\nDRIVER\n\nADAPTER\n\nTWILIO\n\nPASSENGER\nMANAGEMENT\n\neet &\n\nBILLING NOTIFICATION PAYMENTS\n\nQO\n\nTRIP DRIVER\nMANAGEMENT MANAGEMENT\n\nSENDGRID\nADAPTER\n\nSTRIPE\n\neee cece ceeene ADAPTER\n\nFigure 1-1. A sample taxi-hailing application.\n\nAt the core of the application is the business logic, which is implemented by modules\nthat define services, domain objects, and events. Surrounding the core are adapters\nthat interface with the external world. Examples of adapters include database access\ncomponents, messaging components that produce and consume messages, and web\ncomponents that either expose APIs or implement a UI.\n\nMicroservices — From Design to Deployment 2 LT ‘oduction to",
            "status": "success"
        },
        {
            "page_number": 9,
            "text": "Despite having a logically mod\n\ndeployed as a monolith. The actual forma\nand framework. For example, many Java applica\n\ndeployed on application serve\n\npackaged as self-contained executable JARs. Si\n\nare packaged as a directory hi\n\nApplications written in this sty\nsince our IDEs and other tools\nof applications are also simple\nlaunching the application and\n\nular architecture,\n\nio\ns suchas Tomcat o\n\nerarchy.\n\ne are extre\nare focused on build\nto test. You can imp\ntesting the\n\nmi\n\nthe application is packaged and\ndepends on the application's language\n\nns are packaged as WAR files and\nr Jetty. Other Java applications are\nlarly, Rails and Node.js applications\n\nmely common. They are simple to develop\n\ning a single application. These kinds\nement end-to-end testing by simply\n\nUI with a testing package such as Selenium.\n\nMonolithic applications are also simple to deploy. You just have to copy the packaged\n\napplication to a server. You ca\n\nn also scale the appli\n\ncation by running multiple copies\n\nbehind a load balancer. In the early stages of the project it works well.\n\nMarching Toward Monolit\n\nhic Hell\n\nUnfortunately, this simple approach has a huge limitation. Successful applications have\na habit of growing over time and eventually becoming huge. During each sprint, your\n\ndevelopmen\nmany lines o\namonstrous\nwas writing a\nmulti-million\n\nteam implements a few more user stories, which, of course, means adding\ncode. After a few years, your small, simple application will have grown into\nmonolith. To give an extreme example, | recently spoke to a developer who\ntool to analyze the dependencies between the thousands of JARs in their\nines of code (LOC) application. I'm sure it took the concerted effort of a\n\nlarge number of developers over many years to create such a beast.\n\nOnce your app\norganization is\ndelivery will flo\ncomplex. It’s sim\nfixing bugs and\n\np\n\nply\nimp\n\nh\nib\n\nto understand,\nincomprehens\n\ne bi\n\nThe sheer size of th\napplication, the long\nperformance of thei\nas 12 minutes. I've a\n\nAnother problem with a large, complex monolithic application is that it\ncontinuous deployment. Today, the state of the art for SaaS applications\n\nication has become a large, complex mono\nobably in a world of pain. Any attempts at\nunder. One major problem is that the application is overwhelmingly\ntoo large for any single developer to fu\nementing new features correctly bec\nconsuming. What's more, this tends to be a downwards spira\nen changes won't be made correctly. You wi\ng ball of mud.\n\ne application will also slow down deve\ner the start-up time is. | surveyed deve\n\nmonolithic applications, and some reported star\nso heard anecdotes of applications taki\nstart up. If developers regularly have to restart the applicati\nof their day will be spent waiting around and their productivi\n\nith, your development\nagile development and\n\nly understand. As a result,\nomes difficult and time\n\n|. If the codebase is difficult\nll end up with a monstrous,\n\nopment. The larger the\nopers about the size and\nt-up times as long\nng as long as 40 minutes to\non server, then a large part\n\ny will suffer.\n\nis an obstacle to\nis to push changes\n\ninto production many times a day. This is extremely difficult to do with a complex monolith,\n\nMicroservices — From Design to Deployment",
            "status": "success"
        },
        {
            "page_number": 10,
            "text": "since you must redeploy the entire application in order to update any one part of it.\nThe lengthy start-up times that | mentioned earlier won't help either. Also, since the impact\nof a change is usually not very well understood, it is likely that you have to do extensive\nmanual testing. Consequently, continuous deployment is next to impossible to do.\n\nMonolithic applications can also be difficult to scale when different modules have conflicting\nresource requirements. For example, one module might implement CPU-intensive image\nprocessing logic and would ideally be deployed in Amazon EC2 Compute Optimized\ninstances. Another module might be an in-memory database and best suited for EC2\nMemory-optimized instances. However, because these modules are deployed together,\nyou have to compromise on the choice of hardware.\n\nAnother problem with monolithic applications is reliability. Because all modules are running\nwithin the same process, a bug in any module, such as a memory leak, can potentially bring\ndown the entire process. Moreover, since all instances of the application are identical,\nthat bug will impact the availability of the entire application.\n\nLast but not least, monolithic applications make it extremely difficult to adopt new\nframeworks and languages. For example, let's imagine that you have 2 million lines of code\nwritten using the XYZ framework. It would be extremely expensive (in both time and cost)\nto rewrite the entire application to use the newer ABC framework, even if that framework\nwas considerably better. As a result, there is a huge barrier to adopting new technologies.\nYou are stuck with whatever technology choices you made at the start of the project.\n\nTo summarize: you have a successful business-critical application that has grown into a\nmonstrous monolith that very few, if any, developers understand. It is written using obsolete,\nunproductive technology that makes hiring talented developers difficult. The application is\ndifficult to scale and is unreliable. As a result, agile development and delivery of applications\nis impossible.\n\nSo what can you do about it?\n\nMicroservices — Tackling the Complexity\n\nMany organizations, such as Amazon, eBay, and Netflix, have solved this problem by\nadopting what is now known as the Microservices Architecture pattern. Instead of building\na single monstrous, monolithic application, the idea is to split your application into set of\nsmaller, interconnected services.\n\nA service typically implements a set of distinct features or functionality, such as order\nmanagement, customer management, etc. Each microservice is a mini-application that has\nits own hexagonal architecture consisting of business logic along with various adapters.\nSome microservices would expose an API that’s consumed by other microservices or\nby the application's clients. Other microservices might implement a web UI. At runtime,\neach instance is often a cloud virtual machine (VM) or a Docker container.\n\nMicroservices — From Design to Deployment 4 LT ‘oduction to",
            "status": "success"
        },
        {
            "page_number": 11,
            "text": "For example, a possible decomposition of the system described earlier is shown in\nFigure 1-2:\n\nSTRIPE\nADAPTER\n\n>\n\nPASSENGER\nWEB UI\n\nDRIVER\nMANAGEMENT,\n\nTRIP\nMANAGEMENT\n\nDRIVER\nWEB UI\n\nSENDGRID\nADAPTER\n\nFigure 1-2. A monolithic application decomposed into microservices.\n\nEach functional area of the application is now implemented by its own microservice.\nMoreover, the web application is split into a set of simpler web applications — such as\none for passengers and one for drivers, in our taxi-hailing example. This makes it easier\nto deploy distinct experiences for specific users, devices, or specialized use cases.\n\nEach backend service exposes a REST API and most services consume APIs provided by\nother services. For example, Driver Management uses the Notification server to tell an\navailable driver about a potential trip. The Ul services invoke the other services in order to\n\nrender web pages. Services might also use asynchronous, message-based communication.\n\nInter-service communication will be covered in more detail later in this ebook.\n\nMicroservices — From Design to Deployment 5 Ch. 1 — Introduction to Microservices",
            "status": "success"
        },
        {
            "page_number": 12,
            "text": "Some REST APIs are also exposed to the mobile apps used by the drivers and\npassengers. The apps don't, however, have direct access to the backend services.\nInstead, communication is mediated by an intermediary known as an API Gateway.\nThe API Gateway is responsible for tasks such as load balancing, caching, access control,\nAPI metering, and monitoring, and can be implemented effectively using NGINX.\nChapter 2 discusses the API Gateway in detail.\n\nY axis -\nfunctional\ndecomposition\nScale by splitting\ndifferent things\n\nX axis - horizontal duplication\nScale by cloning\n\nFigure 1-3. The Scale Cube, used in both development and delivery.\n\nThe Microservices Architecture pattern corresponds to the Y-axis scaling of the Scale Cube,\nwhich is a 3D model of scalability from the excellent book The Art of Scalability. The other\ntwo scaling axes are X-axis scaling, which consists of running multiple identical copies\nof the application behind a load balancer, and Z-axis scaling (or data partitioning), where an\nattribute of the request (for example, the primary key of a row or identity of a customer)\nis used to route the request to a particular server.\n\nApplications typically use the three types of scaling together. Y-axis scaling decomposes\nthe application into microservices as shown above in Figure 1-2.\n\nMicroservices — From Design to Deployment 6 Ch. 1- oduction to Microservices",
            "status": "success"
        },
        {
            "page_number": 13,
            "text": "At runtime, X-axis scaling runs multiple instances of each service behind a load balancer\nfor throughput and availability. Some applications might also use Z-axis scaling to partition\nthe services. Figure 1-4 shows how the Trip Management service might be deployed\nwith Docker running on Amazon EC2.\n\na\n\nLOAD\nBALANCER\n\nEC2 INSTANCE EC2 INSTANCE\n\n* DOCKER © DOCKER » DOCKER * * DOCKER\n: CONTAINER : } CONTAINER ‘ CONTAINER + 2 CONTAINER\n\nTRIP oo TRIP : : TRIP\n\nMANAGEMENT ay MANAGEMENT : . MANAGEMENT\n\nFigure 1-4. Deploying the Trip Management service using Docker.\n\nAt runtime, the Trip Management service consists of multiple service instances. Each\nservice instance is a Docker container. In order to be highly available, the containers are\nrunning on multiple Cloud VMs. In front of the service instances is a load balancer such\nas NGINX that distributes requests across the instances. The load balancer might also\nhandle other concerns such as caching, access control, AP! metering, and monitoring.\n\nThe Microservices Architecture pattern significantly impacts the relationship between the\napplication and the database. Rather than sharing a single database schema with other\nservices, each service has its own database schema. On the one hand, this approach is\nat odds with the idea of an enterprise-wide data model. Also, it often results in duplication\nof some data. However, having a database schema per service is essential if you want\nto benefit from microservices, because it ensures loose coupling. Figure 1-5 shows the\ndatabase architecture for the sample application.\n\nEach of the services has its own database. Moreover, a service can use a type of database\nthat is best suited to its needs, the so-called polyglot persistence architecture. For example,\nDriver Management, which finds drivers close to a potential passenger, must use a\ndatabase that supports efficient geo-queries.\n\nMicroservices — From Design to Deployment 7 sh. 1 = oduction to M",
            "status": "success"
        },
        {
            "page_number": 14,
            "text": "PASSENGER\nMANAGEMENT\n\nDATABASE\nADAPTER\n\nDRIVER\nMANAGEMENT\n\nDATABASE\nADAPTER\n\nTRIP\nMANAGEMENT\n\nDATABASE\nADAPTER\n\nty @\n\nDRIVER\nMANAGEMENT\nDATABASE\n\n0\n\nTRIP\nMANAGEMENT\nDATABASE\n\npel\n\nPASSENGER\nMANAGEMENT\nDATABASE\n\nFigure 1-5. Database architecture for the taxi-hailing application.\n\nOn the surface, the Microservices Architecture pattern is similar to SOA. With both\napproaches, the architecture consists of a set of services. However, one way to think\nabout the Microservices Architecture pattern is that it's SOA without the commercialization\nand perceived baggage of web service specifications (WS-*) and an Enterprise Service\nBus (ESB). Microservice-based applications favor simpler, lightweight protocols such as\nREST, rather than WS-*. They also very much avoid using ESBs and instead implement\nESB-like functionality in the microservices themselves. The Microservices Architecture\npattern also rejects other parts of SOA, such as the concept of a canonical schema for\ndata access.\n\nThe Benefits of Microservices\n\nThe Microservices Architecture pattern has a number of important benefits. First, it\ntackles the problem of complexity. It decomposes what would otherwise be a monstrous\nmonolithic application into a set of services. While the total amount of functionality is\nunchanged, the application has been broken up into manageable chunks or services.\nEach service has a well-defined boundary in the form of a remote procedure call\n(RPC)-driven or message-driven API. The Microservices Architecture pattern enforces\na level of modularity that in practice is extremely difficult to achieve with a monolithic\ncode base. Consequently, individual services are much faster to develop, and much\neasier to understand and maintain.\n\nMicroservices — From Design to Deployment 8 Ch. 1- oduction to Microservices",
            "status": "success"
        },
        {
            "page_number": 15,
            "text": "Second, this architecture enables each service to be developed independently by a team\nthat is focused on that service. The developers are free to choose whatever technologies\nmake sense, provided that the service honors the API contract. Of course, most\norganizations would want to avoid complete anarchy by limiting technology options.\nHowever, this freedom means that developers are no longer obligated to use the possibly\nobsolete technologies that existed at the start of a new project. When writing a new\nservice, they have the option of using current technology. Moreover, since services are\nrelatively small, it becomes more feasible to rewrite an old service using current technology.\n\nThird, the Microservices Architecture pattern enables each microservice to be deployed\nindependently. Developers never need to coordinate the deployment of changes that are\nocal to their service. These kinds of changes can be deployed as soon as they have been\ntested. The Ul team can, for example, perform AIB testing and rapidly iterate on Ul changes.\nThe Microservices Architecture pattern makes continuous deployment possible.\n\nFinally, the Microservices Architecture pattern enables each service to be scaled\nindependently. You can deploy just the number of instances of each service that satisfy\nits capacity and availability constraints. Moreover, you can use the hardware that best\nmatches a service's resource requirements. For example, you can deploy a CPU-intensive\nimage processing service on EC2 Compute Optimized instances and deploy an in-memory\ndatabase service on EC2 Memory-optimized instances.\n\nThe Drawbacks of Microservices\n\nAs Fred Brooks wrote almost 30 years ago, in The Mythical Man-Month, there are no\nsilver bullets. Like every other technology, the Microservices architecture pattern has\ndrawbacks. One drawback is the name itself. The term microservice places excessive\nemphasis on service size. In fact, there are some developers who advocate for building\nextremely fine-grained 10-100 LOC services. While small services are preferable, it's\nimportant to remember that small services are a means to an end, and not the primary\ngoal. The goal of microservices is to sufficiently decompose the application in order to\nacilitate agile application development and deployment.\n\nAnother major drawback of microservices is the complexity that arises from the fact that\na microservices application is a distributed system. Developers need to choose and\nimplement an inter-process communication mechanism based on either messaging or\nRPC. Moreover, they must also write code to handle partial failure, since the destination\nof a request might be slow or unavailable. While none of this is rocket science, it's much\nmore complex than in a monolithic application, where modules invoke one another via\nanguage-level method/procedure calls.\n\nAnother challenge with microservices is the partitioned database architecture. Business\ntransactions that update multiple business entities are fairly common. These kinds of\ntransactions are trivial to implement in a monolithic application because there is a single\ndatabase. In a microservices-based application, however, you need to update multiple\n\nMicroservices — From Design to Deployment 9 1",
            "status": "success"
        },
        {
            "page_number": 16,
            "text": "databases owned by different services. Using distributed transactions is usually not an\noption, and not only because of the CAP theorem. They simply are not supported by many\n\nof today's highly scalable NoSQL\n\ndatabases and messaging brokers. You end up having to\n\nuse an eventual consistency-based approach, which is more challenging for developers.\n\nTesting a microservices application is also much more complex. For example, with a modern\nframework such as Spring Boot, it is trivial to write a test class that starts up a monolithic\nweb application and tests its REST API. In contrast, a similar test class for a service\n\nwould need to launch that servi\n\nce and any services that it depends upon, or at least\n\nconfigure stubs for those services. Once again, this is not rocket science, but it’s important\n\nto not underestimate the complexity of doing this.\n\nAnother major challenge with the Microservices Architecture pattern is implementing\nchanges that span multiple services. For example, let's imagine that you are implementing\na story that requires changes to services A, B, and C, where A depends upon B and B\n\ndepends upon C. Ina monolithic\n\napplication you could simply change the corresponding\n\nmodules, integrate the changes, and deploy them in one go. In contrast, in a Microservices\nArchitecture pattern you need to carefully plan and coordinate the rollout of changes\nto each of the services. For example, you would need to update service C, followed by\nservice B, and then finally service A. Fortunately, most changes typically impact only\none service; multi-service changes that require coordination are relatively rare.\n\nDeploying a microservices-based application is also much more complex. A monolithic\napplication is simply deployed on a set of identical servers behind a traditional load balancer.\n\nEach application instance is confi\n\ngured with the locations (host and ports) of infrastructure\n\nservices such as the database and a message broker. In contrast, a microservice\napplication typically consists of a large number of services. For example, Hailo has\n160 different services and Netflix has more than 600, according to Adrian Cockcroft.\n\nEach service will have multiple runtime instances. That's many more moving parts that\nneed to be configured, deployed, scaled, and monitored. In addition, you will also need to\n\nimplement a service discovery mechanism that enables a service to discover the locations\n\n(hosts and ports) of any other se\n\nticket-based and manual approaches to operations cannot scale to this level of complexity.\nConsequently, successfully deploying a microservices application requires greater control\n\nof deployment methods by deve\n\nOne approach to automation is to use an off-the-shelf platform-as-a-service (PaaS) such\n\nas Cloud Foundry. A PaaS provid\ntheir microservices. It insulates t\n\nvices it needs to communicate with. Traditional trouble\n\nopers and a high level of automation.\n\nes developers with an easy way to deploy and manage\nhem from concerns such as procuring and configuring\n\nIT resources. At the same time, the systems and network professionals who configure\nthe PaaS can ensure compliance with best practices and with company policies.\n\nAnother way to automate the deployment of microservices is to develop what is\nessentially your own Paas. One typical starting point is to use a clustering solution, such\nas Kubernetes, in conjunction with a container technology such as Docker. Later in this\n\nMicroservices — From Design to Deployment",
            "status": "success"
        },
        {
            "page_number": 17,
            "text": "ebook we will look at how software-based application delivery approaches like NGINX,\nwhich easily handles caching, access control, API metering, and monitoring at the\nmicroservice level, can help solve this problem.\n\nSummary\n\nBuilding complex applications is inherently difficult. The Monolithic Architecture pattern\nonly makes sense for simple, lightweight applications. You will end up in a world of pain\nif you use it for complex applications. The Microservices Architecture pattern is\nthe better choice for complex, evolving applications, despite the drawbacks and\nimplementation challenges.\n\nIn later chapters, I'll dive into the details of various aspects of the Microservices Architecture\npattern and discuss topics such as service discovery, service deployment options,\nand strategies for refactoring a monolithic application into services.\n\nMicroservices in Action: NGINX Plus as a Reverse Proxy Server\nby Floyd Smith\n\nNGINX powers more than 50% of the top 10,000 websites, and that's largely because of its\ncapabilities as a reverse proxy server. You can “drop NGINX in front of\" current applications and\neven database servers to gain all sorts of capabilities — higher performance, greater security,\nscalability, flexibility, and more. All with little or no change to your existing application and\nconfiguration code. For sites suffering performance stress — or anticipating high loads in the future —\nthe effect may seem little short of miraculous.\n\nSo what does this have to do with microservices? Implementing a reverse proxy server, and using\nthe other capabilities of NGINX, gives you architectural flexibility. A reverse proxy server, static\nand application file caching, and SSL/TLS and HTTP/2 termination all take load off your application,\nfreeing it to “do what only it” — the application — “can do”.\n\nNGINX also serves as a load balancer, a crucial role in microservices implementations. The ad-\nvanced features in NGINX Plus, including sophisticated load-balancing algorithms, multiple\nmethods for session persistence, and management and monitoring, are especially useful wi\nmicroservices. (NGINX has recently added support for service discovery using DNS SRV record\na cutting-edge feature.) And, as mentioned in this chapter, NGINX can help in automating th\ndeployment of microservices.\n\noS\n\nIn addition, NGINX provides the necessary functionality to power the three models in the NGINX\nMicroservices Reference Architecture. The Proxy Model uses NGINX as an API Gateway; the Router\nMesh model uses an additional NGINX server as a hub for inter-process communication; and the\nFabric Model uses one NGINX server per microservice, controlling HTTP traffic and, optionally,\nimplementing SSL/TLS between microservices, a breakthrough capability.\n\nMicroservices — From Design to Deployment 11 1",
            "status": "success"
        },
        {
            "page_number": 18,
            "text": "Using an\nAPI Gateway\n\nThe first chapter in this seven-chapter book about designing, building, and deploying\nmicroservices introduced the Microservices Architecture pattern. It discussed the\nbenefits and drawbacks of using microservices and how, despite the complexity of\nmicroservices, they are usually the ideal choice for complex applications. This is the\nsecond article in the series and will discuss building microservices using an API Gateway.\n\nWhen you choose to build your application as a set of microservices, you need to decide\nhow your application's clients will interact with the microservices. With a monolithic\napplication there is just one set of endpoints, which are typically replicated, with load\nbalancing used to distribute traffic among them.\n\nIn a microservices architecture, however, each microservice exposes a Set of what are\ntypically fine-grained endpoints. In this article, we examine how this impacts client-to-\napplication communication and propose an approach that uses an API Gateway.\n\nIntroduction\n\nLet's imagine that you are developing a native mobile client for a shopping application.\nIt's likely that you need to implement a product details page, which displays information\nabout any given product.\n\nFor example, Figure 2-1 shows what you will see when scrolling through the product\ndetails in Amazon's Android mobile application.\n\nMicroservices — From Design to Deployment 12 2- ganA",
            "status": "success"
        },
        {
            "page_number": 19,
            "text": "Cee\n\n= amazon Wy = amare Vy a\n\nnenet nee |) ie ® Frequently bought together\n\nPoo es 1. ORDER HISTORY\na 2. REVIEWS\n\ne. 3. BASIC PRODUCT INFO\n\n2 @ 4, RECOMMENDATION\nmn - 5. INVENTORY\n\n=o 6. SHIPPING\n\nFigure 2-1. A sample shopping application.\n\nEven though this is a smartphone application, the product details page displays a lot of\ninformation. For example, not only is there basic product information, such as name,\ndescription, and price, but this page also shows:\n\n. Number of items in the shopping cart\n\n. Order history\n\n. Customer reviews\n\n. Low inventory warning\n\n. Shipping options\n\n. Various recommendations, including other products this product is frequently\nbought with, other products bought by customers who bought this product, and\nother products viewed by customers who bought this product\n\n7. Alternative purchasing options\n\nOoRWN >\n\nWhen using a monolithic application architecture, a mobile client retrieves this data by\nmaking a single REST call to the application, such as:\n\nGET api.company.com/productdetails/productId\n\nA load balancer routes the request to one of several identical application instances.\nThe application then queries various database tables and return the response to the client.\n\nMicroservices — From Design to Deployment 13 Sh. 2-U",
            "status": "success"
        },
        {
            "page_number": 20,
            "text": "In contrast, when using the microservices architecture, the data displayed on the\nproduct details page is owned by multiple microservices. Here are some of the potential\nmicroservices that own data displayed on the sample product-specific page:\nShopping Cart Service — Number of items in the shopping cart\n\nOrder Service — Order history\n\nCatalog Service — Basic product information, such as product name, image, and price\n* Review Service —- Customer reviews\n\nInventory Service — Low inventory warning\n\nShipping Service — Shipping options, deadlines, and costs, drawn separately from the\nshipping provider's API\n\n*« Recommendation Service(s) — Suggested items\n\nCART\nAVICE\n\nIMENDAI\nSERVICE\n\nORDER\nSERVICE\n\nPRODUCT\nCATALOG\nSERVICE\n\nFigure 2-2. Mapping a mobile client's needs to relevant microservices.\n\nWe need to decide how the mobile client accesses these services. Let's look at the options.\n\nMicroservices — From Design to Deployment 14\n\nCh. 2— Using an API Gateway",
            "status": "success"
        },
        {
            "page_number": 21,
            "text": "Direct Client-to-Microservice Communication\n\nIn theory, a client could make requests to each of the microservices directly.\nEach microservice would have a public endpoint:\n\nhttps://serviceName.api.company.name\n\nThis URL would map to the microservice's load balancer, which distributes requests\nacross the available instances. To retrieve the product-specific page information,\nthe mobile client would make requests to each of the services listed above.\n\nUnfortunately, there are challenges and limitations with this option. One problem is the\nmismatch between the needs of the client and the fine-grained APls exposed by each\nof the microservices. The client in this example has to make seven separate requests.\nIn more complex applications it might have to make many more. For example, Amazon\ndescribes how hundreds of services are involved in rendering their product page.\nWhile a client could make that many requests over a LAN, it would probably be too\ninefficient over the public Internet and would definitely be impractical over a mobile\nnetwork. This approach also makes the client code much more complex.\n\nAnother problem with the client directly calling the microservices is that some might use\n\nprotocols that are not web-friendly. One service might use Thrift binary RPC while another\nservice might use the AMQP messaging protocol. Neither protocol is particularly browser-\nor firewall-friendly, and is best used internally. An application should use protocols such\n\nas HTTP and WebSocket outside of the firewall.\n\nAnother drawback with this approach is that it makes it difficult to refactor the microservices.\nOver time we might want to change how the system is partitioned into services. For example,\nwe might merge two services or split a service into two or more services. If, however,\nclients communicate directly with the services, then performing this kind of refactoring\ncan be extremely difficult.\n\nBecause of these kinds of problems it rarely makes sense for clients to talk directly\nto microservices.\n\nUsing an API Gateway\n\nUsually a much better approach is to use what is known as an API Gateway. An API\nGateway is a server that is the single entry point into the system. It is similar to the Facade\npattern from object-oriented design. The API Gateway encapsulates the internal system\narchitecture and provides an API that is tailored to each client. It might have other\nresponsibilities such as authentication, monitoring, load balancing, caching, request\nshaping and management, and static response handling.\n\nMicroservices — From Design to Deployment 15 2- ganA",
            "status": "success"
        },
        {
            "page_number": 22,
            "text": "Figure 2-3 shows how an API Gateway typically fits into the architecture.\n\nCATALOG\nSERVICE\n\nFigure 2-3. Using an API Gateway with microservices.\n\nThe API Gateway is responsible for request routing, composition, and protocol translation.\nAll requests from clients first go through the API Gateway. It then routes requests to the\nappropriate microservice. The API Gateway will often handle a request by invoking multiple\nmicroservices and aggregating the results. It can translate between web protocols such\nas HTTP and WebSocket and web-unfriendly protocols that are used internally.\n\nThe API Gateway can also provide each client with a custom API. It typically exposes a\ncoarse-grained API for mobile clients. Consider, for example, the product details scenario.\nThe API Gateway can provide an endpoint (/productdetails?productid=xxx) that\n\nMicroservices — From Design to Deployment 16 Ch. 2— Using an API Gateway",
            "status": "success"
        },
        {
            "page_number": 23,
            "text": "enables a mobile client to retrieve all of the product details with a single request. The API\nGateway handles the request by invoking the various services — product information,\nrecommendations, reviews, etc. - and combining the results.\n\nA great example of an API Gateway is the Netflix API Gateway. The Netflix streaming\nservice is available on hundreds of different kinds of devices including televisions,\nset-top boxes, smartphones, gaming systems, tablets, etc. Initially, Netflix attempted\n\nto provide a one-size-fits-all API for their streaming service. However, they discovered\nthat it didn’t work well because of the diverse range of devices and their unique needs.\nToday, they use an API Gateway that provides an API tailored for each device by running\ndevice-specific adapter code. An adapter typically handles each request by invoking,\non average, six to seven backend services. The Netflix API Gateway handles billions of\nrequests per day.\n\nBenefits and Drawbacks of an API Gateway\n\nAs you might expect, using an API Gateway has both benefits and drawbacks. A major\nbenefit of using an API Gateway is that it encapsulates the internal structure of the\napplication. Rather than having to invoke specific services, clients simply talk to the\ngateway. The API Gateway provides each kind of client with a specific API. This reduces the\nnumber of round trips between the client and application. It also simplifies the client code.\n\nThe API Gateway also has some drawbacks. It is yet another highly available component\nthat must be developed, deployed, and managed. There is also a risk that the API Gateway\nbecomes a development bottleneck. Developers must update the AP! Gateway in order\nto expose each microservice's endpoints.\n\n[tis important that the process for updating the API Gateway be as lightweight as possible.\nOtherwise, developers will be forced to wait in line in order to update the gateway. Despite\nthese drawbacks, however, for most real-world applications it makes sense to use an\nAPI Gateway.\n\nImplementing an API Gateway\n\nNow that we have looked at the motivations and the trade-offs for using an API Gateway,\nlet's look at various design issues you need to consider.\n\nPerformance and Scalability\n\nOnly a handful of companies operate at the scale of Netflix and need to handle billions\nof requests per day. However, for most applications the performance and scalability of\nthe API Gateway is usually very important. It makes sense, therefore, to build the API\nGateway on a platform that supports asynchronous, non-blocking I/O. There are a variety\nof different technologies that can be used to implement a scalable API Gateway. On the\nJVM you can use one of the NlIO-based frameworks such Netty, Vertx, Spring Reactor,\n\nMicroservices — From Design to Deployment 17 2- ganA",
            "status": "success"
        },
        {
            "page_number": 24,
            "text": "or JBoss Undertow. One popular non-JVM option is Node,js, which is a platform built on\nChrome's JavaScript engine. Another option is to use NGINX Plus.\n\nNGINX Plus offers a mature, scalable, high-performance web server and reverse proxy that\nis easily deployed, configured, and programmed. NGINX Plus can manage authentication,\naccess control, load balancing requests, caching responses, and provides application-\naware health checks and monitoring.\n\nUsing a Reactive Programming Model\n\nThe API Gateway handles some requests by simply routing them to the appropriate\nbackend service. It handles other requests by invoking multiple backend services and\naggregating the results. With some requests, such as a product details request, the\nrequests to backend services are independent of one another. In order to minimize\nresponse time, the API Gateway should perform independent requests concurrently.\n\nSometimes, however, there are dependencies between requests. The API Gateway\nmight first need to validate the request by calling an authentication service before\nrouting the request to a backend service. Similarly, to fetch information about the\nproducts in a customer's wish list, the API Gateway must first retrieve the customer's\nprofile containing that information, and then retrieve the information for each product.\nAnother interesting example of AP! composition is the Netflix Video Grid.\n\nWriting API composition code using the traditional asynchronous callback approach\nquickly leads you to callback hell. The code will be tangled, difficult to understand, and\nerror-prone. A much better approach is to write API Gateway code in a declarative style\nusing a reactive approach. Examples of reactive abstractions include Future in Scala,\nCompletableFuture in Java 8, and Promise in JavaScript. There is also Reactive Extensions\n(also called Rx or Reactivex), which was originally developed by Microsoft for the NET\nplatform. Netflix created RxJava for the UVM specifically to use in their AP! Gateway.\nThere is also RxJS for JavaScript, which runs in both the browser and Node,js. Using a\nreactive approach will enable you to write simple yet efficient API Gateway code.\n\nService Invocation\n\nA microservices-based application is a distributed system and must use an\ninter-process communication mechanism. There are two styles of inter-process\ncommunication. One option is to use an asynchronous, messaging-based mechanism.\nSome implementations use a message broker such as JMS or AMQP. Others, such\nas Zeromg, are brokerless and the services communicate directly.\n\nThe other style of inter-process communication is a synchronous mechanism such as\n\nHTTP or Thrift. A system will typically use both asynchronous and synchronous styles.\n\nIt might even use multiple implementations of each style. Consequently, the API Gateway\nwill need to support a variety of communication mechanisms.\n\nMicroservices — From Design to Deployment 18 2- ganA",
            "status": "success"
        },
        {
            "page_number": 25,
            "text": "Service Discovery\n\nThe API Gateway needs to know the location (IP address and port) of each microservice\nwith which it communicates. In a traditional application, you could probably hardwire the\nlocations, but ina modern, cloud-based microservices application, finding the needed\n\nlocations is a non-trivial problem.\n\nInfrastructure services, such as a message broker, will usually have a static location,\nwhich can be specified via OS environment variables. However, determining the location\nof an application service is not So easy.\n\nApplication services have dynamically assigned locations. Also, the set of instances\nof a service changes dynamically because of autoscaling and upgrades. Consequently,\nthe API Gateway, like any other service client in the system, needs to use the system's\nservice discovery mechanism: either server-side discovery or client-side discovery.\nChapter 4 describes service discovery in more detail. For now, it is worthwhile to note that\nif the system uses client-side discovery, then the API Gateway must be able to query the\nservice registry, which is a database of all microservice instances and their locations.\n\nHandling Partial Failures\n\nAnother issue you have to address when implementing an AP! Gateway is the problem of\npartial failure. This issue arises in all distributed systems whenever one service calls another\nservice that is either responding slowly or is unavailable. The API Gateway should never\nblock indefinitely waiting for a downstream service. However, how it handles the failure\ndepends on the specific scenario and which service is failing. For example, if the\nrecommendation service is unresponsive in the product details scenario, the API Gateway\nshould return the rest of the product details to the client since they are still useful to\nthe user. The recommendations could either be empty or replaced by, for example,\n\na hardwired top ten list. If, however, the product information service is unresponsive,\nthen the API Gateway should return an error to the client.\n\nThe API Gateway could also return cached data if that is available. For example, since\nproduct prices change infrequently, the API Gateway could return cached pricing data if\nthe pricing service is unavailable. The data can be cached by the API Gateway itself or\nbe stored in an external cache, such as Redis or Memcached. By returning either default\ndata or cached data, the API Gateway ensures that system failures minimally impact the\nuser experience.\n\nNetflix Hystrix is an incredibly useful library for writing code that invokes remote services.\nHystrix times out calls that exceed the specified threshold. It implements a circuit breaker\npattern, which stops the client from waiting needlessly for an unresponsive service. If the\nerror rate for a service exceeds a specified threshold, Hystrix trips the circuit breaker\nand all requests will fail immediately for a specified period of time. Hystrix lets you define\na fallback action when a request fails, such as reading from a cache or returning a default\nvalue. If you are using the JVM you should definitely consider using Hystrix. And, if you\nare running in a non-JVM environment, you should use an equivalent library.\n\nMicroservices — From Design to Deployment 19 2- ganA",
            "status": "success"
        },
        {
            "page_number": 26,
            "text": "Summary\n\nFor most microservices-based applications, it makes sense to implement an API Gateway,\nwhich acts as a single entry point into a system. The API Gateway is responsible for request\nrouting, composition, and protocol translation. It provides each of the application's clients\nwith a custom API. The API Gateway can also mask failures in the backend services by\nreturning cached or default data. In the next chapter, we will look at communication\nbetween services.\n\nMicroservices in Action: NGINX Plus as an API Gateway\nby Floyd Smith\n\nThis chapter discusses how an API Gateway serves as a single entry point into a system. Andit can\nhandle other functions such as load balancing, caching, monitoring, protocol translation, and others —\nwhile NGINX, when implemented as a reverse proxy server, functions as a single entry point into\na system and supports all the additional functions mentioned for an API Gateway. So using NGINX\nas the host for an API Gateway can work very well indeed.\n\nThinking of NGINX as an API Gateway is not an idea that's original to this ebook. NGINX Plus is a\nleading platform for managing and securing HT TP-based API traffic. You can implement your own\nAPI Gateway or use an existing AP! management platform, many of which leverage NGINX.\n\nReasons for using NGINX Plus as an AP! Gateway include:\n\n* Access management - You can use a variety of access control list (ACL) methods and easily\nimplement SSL/TLS, either at the web application level as is typical, or also down to the level\nof each individual microservice.\n\n* Manageability and resilience — You can update your NGINX Plus-based API server without\ndowntime, using the NGINX dynamic reconfiguration API, a Lua module, Perl, live restarts without\ndowntime, or changes driven by Chef, Puppet, ZooKeeper, or DNS.\n\n¢ Integration with third-party tools — NGINX Plus is already integrated with leading-edge tools\nsuch as 3scale, Kong, and the MuleSoft integration platform (to mention only tools described\non the NGINX website.)\n\nNGINX Plus is used extensively as an API Gateway in the NGINX Microservices Reference Archi-\ntecture. Use the articles assembled here and, when publicly available, the MRA, for examples of\nhow to implement this in your own applications.\n\nMicroservices — From Design to Deployment 20 )",
            "status": "success"
        },
        {
            "page_number": 27,
            "text": "Inter-Process\nCommunication\n\nThis is the third chapter in this ebook about building applications with a microservices\narchitecture. Chapter 1 introduces the Microservices Architecture pattern, compares\nit with the Monolithic Architecture pattern, and discusses the benefits and drawbacks\nof using microservices. Chapter 2 describes how clients of an application communicate\nwith the microservices via an intermediary known as an API Gateway. In this chapter,\nwe take a look at how the services within a system communicate with one another.\nChapter 4 explores the closely related problem of service discovery.\n\nIntroduction\n\nIn a monolithic application, components invoke one another via language-level method\nor function calls. In contrast, a microservices-based application is a distributed system\nrunning on multiple machines. Each service instance is typically a process.\n\nConsequently, as Figure 3-1 shows, services must interact using an inter-process\ncommunication (IPC) mechanism.\n\nLater on we will look at specific IPC technologies, but first let's explore various design issues.\n\nMicroservices — From Design to Deployment 21 Ch. 3 - Inter-Process Communicatic",
            "status": "success"
        },
        {
            "page_number": 28,
            "text": "STRIPE\nADAPTER\n\nPASSENGER\nMANAGEMENT\n\nDRIVER\nMANAGEMENT,\n\nsitunc | |NotiFication |\\| PAYMENTS.\n\nDL att riven\nwanacement| | wanacement | ()\n\nTRIP\nMANAGEMENT\n\nSENDGRID\n\"ADAPTER\n\nFigure 3-1. Microservices use inter-process communication to interact.\n\nInteraction Styles\n\nWhen selecting an IPC mechanism for a service, it is useful to think first about how services\ninteract. There are a variety of clienteservice interaction styles. They can be categorized\nalong two dimensions. The first dimension is whether the interaction is one-to-one or\none-to-many:\n\n* One-to-one — Each client request is processed by exactly one service instance.\n\n* One-to-many — Each request is processed by multiple service instances.\n\nThe second dimension is whether the interaction is synchronous or asynchronous:\n\n* Synchronous — The client expects a timely response from the service and might even\nblock while it waits.\n\ne Asynchronous — The client doesn't block while waiting for a response, and the\nresponse, if any, isn’t necessarily sent immediately.\n\nThe following table shows the various interaction styles.\n\nONE-TO-ONE ONE-TO-MANY\n\nSYNCHRONOUS Request/response —_\n\nNotification Publish/subscribe\nASYNCHRON -\nRequest/async response Publish/async responses\n\nTable 3-1. Inter-process communication styles.\n\nMicroservices — From Design to Deployment 22 Ch. 3 -Inter-P Communicatio",
            "status": "success"
        },
        {
            "page_number": 29,
            "text": "There are the following kinds of one-to-one interactions, both synchronous (request/\nresponse) and asynchronous (notification and request/async response):\n\n« Request/response — A client makes a request to a service and waits for a response.\nThe client expects the response to arrive in a timely fashion. In a thread-based\napplication, the thread that makes the request might even block while waiting.\n\n* Notification (a.k.a. a one-way request) — A client sends a request to a service but\nno reply is expected or sent.\n\n« Request/async response — A client sends a request to a service, which replies\nasynchronously. The client does not block while waiting and is designed with the\nassumption that the response might not arrive for a while.\n\nThere are the following kinds of one-to-many interactions, both of which are asynchronous:\n\n¢ Publish/subscribe — A client publishes a notification message, which is consumed by\nzero or more interested services.\n\n¢ Publish/async responses — A client publishes a request message, and then waits a\ncertain amount of time for responses from interested services.\n\nEach service typically uses a combination of these interaction styles. For some services,\na single IPC mechanism is sufficient. Other services might need to use a combination of\nIPC mechanisms.\n\nFigure 3-2 shows how services in a taxi-hailing application might interact when the user\nrequests a trip.\n\nPASSENGER\nMANAGEMENT\n\n2 REQUEST/RESPONSE\n\nPASSENGER\n\n5\n\nNOTIFICATION\n\n4 PUB/SUB\n\n1 NOTIFICATION\n\nPASSENGER (\")\nSMARTPHON!\n\n3 PUB/SUB\n\n4 PUB/SUB NOTIFICATION\n\nDRIVER\nMANAGEMENT\n\nFigure 3-2. Using multiple IPC mechanisms for service interactions.\n\nMicroservices — From Design to Deployment 23 Ch, 3 -Inter-Pr\n\n3 Communicatio",
            "status": "success"
        },
        {
            "page_number": 30,
            "text": "The services use a combination of notifications, request/response, and publish/subscribe.\nFor example, the passenger's smartphone sends a notification to the Trip Management\nservice to request a pickup. The Trip Management service verifies that the passenger's\naccount is active by using request/response to invoke the Passenger Management\nservice. The Trip Management service then creates the trip and uses publish/subscribe\nto notify other services including the Dispatcher, which locates an available driver.\n\nNow that we have looked at interaction styles, let's take a look at how to define APIs.\n\nDefining APls\n\nAservice's API is a contract between the service and its clients. Regardless of your choice\nof IPC mechanism, it's important to precisely define a service's API using some kind of\ninterface definition language (IDL). There are even good arguments for using an API-first\napproach to defining services. You begin the development of a service by writing the\ninterface definition and reviewing it with the client developers. It is only after iterating on\nthe API definition that you implement the service. Doing this design up front increases\nyour chances of building a service that meets the needs of its clients.\n\nAs you will see later in this article, the nature of the API definition depends on which IPC\nmechanism you are using. If you are using messaging, the API consists of the message\nchannels and the message types. If you are using HTTP, the API consists of the URLs and\nthe request and response formats. Later on we will describe some IDLs in more detail.\n\nEvolving APIs\n\nA service's API invariably changes over time. In a monolithic application it is usually\nstraightforward to change the API and update all the callers. In a microservices-based\napplication it is a lot more difficult, even if all of the consumers of your API are other\nservices in the same application. You usually cannot force all clients to upgrade in lockstep\nwith the service. Also, you will probably incrementally deploy new versions of a service\nsuch that both old and new versions of a service will be running simultaneously. It is\nimportant to have a strategy for dealing with these issues.\n\nHow you handle an API change depends on the size of the change. Some changes are\nminor and backward compatible with the previous version. You might, for example, add\nattributes to requests or responses. It makes sense to design clients and services so\nthat they observe the robustness principle. Clients that use an older API should continue\nto work with the new version of the service. The service provides default values for the\nmissing request attributes and the clients ignore any extra response attributes. It is\nimportant to use an IPC mechanism and a messaging format that enable you to easily\nevolve your APIs.\n\nSometimes, however, you must make major, incompatible changes to an API. Since you\ncan't force clients to upgrade immediately, a service must support older versions of the\n\nMicroservices — From Design to Deployment 24 1. 3-",
            "status": "success"
        },
        {
            "page_number": 31,
            "text": "API for some period of time. If you are using an HTTP-based mechanism such as REST,\none approach is to embed the version number in the URL. Each service instance might\nhandle multiple versions simultaneously. Alternatively, you could deploy different\ninstances that each handle a particular version.\n\nHandling Partial Failure\n\nAs mentioned in Chapter 2 about the API Gateway, in a distributed system there is the\never-present risk of partial failure. Since clients and services are separate processes,\naservice might not be able to respond in a timely way to a client's request. A service might\nbe down because of a failure or for maintenance. Or the service might be overloaded\nand responding extremely slowly to requests.\n\nConsider, for example, the product details scenario from Chapter 2. Let's imagine that the\nRecommendation Service is unresponsive. A naive implementation of a client might block\nindefinitely waiting for a response. Not only would that result in a poor user experience,\nbut also, in many applications it would consume a precious resource such as a thread.\nEventually the runtime would run out of threads and become unresponsive, as shown in\n\nFigure 3-3.\nIF THE SERVICE IS\nTOMCAT DOWN THEN THREAD\nrare | WILL BE BLOCKED\nTHAD 1 SS\nHTTP REQUEST : THRRAD 2 >\n\n—_ poet RPC CLIENT CODE\n: THAAD JS\n\nEXECUTE THREAD POOL\n\nEVENTUALLY ALL THREADS WILL BE BLOCKED\n\nFigure 3-3. Threads block due to an unresponsive service.\n\nTo prevent this problem, it is essential that you design your services to handle partial failures.\n\nMicroservices — From Design to Deployment 25 Ch. 3 -Inter-Proc C lunicat",
            "status": "success"
        },
        {
            "page_number": 32,
            "text": "A good approach to follow is the one described by Netflix. The strategies for dealing\nwith partial failures include:\n\nNetwork timeouts — Never block indefinitely and always use timeouts when waiting\nfor a response. Using timeouts ensures that resources are never tied up indefinitely.\nLimiting the number of outstanding requests — Impose an upper bound on the number\nof outstanding requests that a client can have with a particular service. If the limit has\nbeen reached, it is probably pointless to make additional requests, and those attempts\nneed to fail immediately.\nCircuit breaker pattern — Track the number of successful and failed requests. If the error\nrate exceeds a configured threshold, trip the circuit breaker so that further attempts\nfail immediately. If a large number of requests are failing, that suggests the service is\nunavailable and that sending requests is pointless. After a timeout period, the client\nshould try again and, if successful, close the circuit breaker.\n\n° Provide fallbacks — Perform fallback logic when a request fails. For example, return\ncached data or a default value, such as an empty set of recommendations.\n\nNetflix Hystrix is an open source library that implements these and other patterns. If you\nare using the JVM you should definitely consider using Hystrix. And, if you are running in\nanon-JVM environment, you should use an equivalent library.\n\nIPC Technologies\n\nThere are lots of different IPC technologies to choose from. Services can use synchronous\nrequest/response-based communication mechanisms such as HTTP-based REST\nor Thrift. Alternatively, they can use asynchronous, message-based communication\nmechanisms such as AMQP or STOMP.\n\nThere are also a variety of different message formats. Services can use human readable,\ntext-based formats such as JSON or XML. Alternatively, they can use a binary format\n(which is more efficient) such as Avro or Protocol Buffers. Later on we will look at\nsynchronous IPC mechanisms, but first let's discuss asynchronous IPC mechanisms.\n\nAsynchronous, Message-Based Communication\n\nWhen using messaging, processes communicate by asynchronously exchanging\nmessages. A client makes a request to a service by sending it a message. If the service\nis expected to reply, it does so by sending a separate message back to the client.\nSince the communication is asynchronous, the client does not block waiting for a reply.\nInstead, the client is written assuming that the reply will not be received immediately.\n\nMicroservices — From Design to Deployment 26 1. 3-",
            "status": "success"
        },
        {
            "page_number": 33,
            "text": "A message consists of headers (metadata such as the sender) and a message body.\nMessages are exchanged over channels. Any number of producers can send messages\nto a channel. Similarly, any number of consumers can receive messages from a channel.\nThere are two kinds of channels, point-to-point and publish-subscribe:\n¢ Apoint-to-point channel delivers a message to exactly one of the consumers that are\nreading from the channel. Services use point-to-point channels for the one-to-one\ninteraction styles described earlier.\n¢ Apublish-subscribe channel delivers each message to all of the attached consumers.\nServices use publish-subscribe channels for the one-to-many interaction styles\ndescribed above.\n\nFigure 3-4 shows how the taxi-hailing application might use publish-subscribe channels.\n\nTRIP CREATED\n\nTRIP ‘oe NEW TRIPS - PUBLISH/SUBSCRIBE CHANNEL || DISPATCHER\n\nMANAGEMENT, .\n\nDRIVER PROPOSED\n\nDISPATCHING - PUBLISH/SUBSCRIBE CHANNEL\n\nPASSENGER\nMANAGEMENT\n\nDRIVER\nMANAGEMENT\n\nFigure 3-4. Using publish-subscribe channels in a taxi-hailing application.\n\nThe Trip Management service notifies interested services, such as the Dispatcher, about a\nnew Trip by writing a Trip Created message to a publish-subscribe channel. The Dispatcher\nfinds an available driver and notifies other services by writing a Driver Proposed message\nto a publish-subscribe channel.\n\nMicroservices — From Design to Deployment 27 Ch. 3 - Inte\n\nCommunicatio",
            "status": "success"
        },
        {
            "page_number": 34,
            "text": "There are many messaging systems to choose from. You should pick one that supports\na variety of programming languages.\n\nSome messaging systems support standard protocols such as AMQP and STOMP.\nOther messaging systems have proprietary but documented protocols.\n\nThere are a large number of open source messaging systems to choose from, including\nRabbitMQ, Apache Kafka, Apache ActiveMQ, and NSQ. Ata high level, they all support\nsome form of messages and channels. They all strive to be reliable, high-performance,\nand scalable. However, there are significant differences in the details of each broker's\n\nmessagi\n\nThere are man\n* Decouples\n\nnstances. |\nof a service instance.\n\nng model.\n\n° Message buffering — Wi\n\nthe cli\names\nproce\ncusto\n\n° Flexib\ndescr\n° Explic\n\nsage broker queu\n\ne client-service i\nibed earlier.\n\nnvoki\n\nof the\n\ne up.\nnteractions —\n\nit inter-process communication\nng aremote servi\n\nh a synchronou\nent and service must be availab\n\ny advantages to using messaging:\n\nhe client from the service — A client makes a request simply by sending a\nmessage to the appropriate channel. The client is completely unaware of the service\ndoes not need to use a discovery mechanism to determine the location\n\ns request/response protocol, such as HTTP, both\ne for the duration of the exchange. In contrast,\n\nes up the messages written to a channel until the consumer can\nss them. This means, for examp\nmers even when the order fulfillment system is slow or unavailable. The order\nmessages simply queu\n\ne, that an online store can accept orders from\n\nessaging supports all of the interaction styles\n\n—RPC-based mechanisms attempt to make\n\nce look the same as calling a local service. However, because\naws of physics and the possibility of partial failure, they are in fact quite different.\nessaging makes these differences very explicit so developers are not lulled into a\nfalse sense of security.\n\nThere are, however, some downsides to using messaging:\n\n¢ Additional operational complexity — The messaging system is yet another system\ncomponent that must be installed, configured, and operated. It's essential that the\nmessage broker be highly available, otherwise system reliability is impacted.\n\nstyle interaction requires some work\ncontain a reply channel identifier and\n\nresponse message containing the cor\n\nComplexity of implementing request/response-based interaction — Request/response-\n\no implement. Each request message must\na correlation identifier. The service writes a\nelation ID to the reply channel. The client uses\n\nthe correlation ID to match the response with the request. It is often easier to use an\nIPC mechanism that directly supports\n\nNow that we have looked at using messag\n\nbased IP’\n\nC.\n\nrequest/response.\n\ning-based IPC, let's examine request/response-\n\nMicroservices - From\n\nDesign to Deployment\n\n28 1.3 =",
            "status": "success"
        },
        {
            "page_number": 35,
            "text": "Synchronous, Request/Response IPC\n\nWhen using a synchronous, request/response-based IPC mechanism, a client sends a\nrequest to a service. The service processes the request and sends back a response.\n\nIn many clients, the thread that makes the request blocks while waiting for a response.\nOther clients might use asynchronous, event-driven client code that is perhaps\nencapsulated by Futures or Rx Observables. However, unlike when using messaging,\nthe client assumes that the response will arrive in a timely fashion.\n\nThere are numerous protocols to choose from. Two popular protocols are REST and Thrift.\nLet's first take a look at REST.\n\nREST\n\nToday it is fashionable to develop APIs in the RESTful style. REST is an IPC mechanism\nthat (almost always) uses HTTP.\n\nA key concept in REST is a resource, which typically represents a business object such\nas a Customer or Product, or a collection of such business objects. REST uses the HTTP\nverbs for manipulating resources, which are referenced using a URL. For example, a GET\nrequest returns the representation of a resource, which might be in the form of an XML\ndocument or JSON object. A POST request creates a new resource, and a PUT request\nupdates a resource.\n\nTo quote Roy Fielding, the creator of REST:\n\n“REST provides a set of architectural constraints that, when applied as a whole, emphasizes\nscalability of component interactions, generality of interfaces, independent deployment of\ncomponents, and intermediary components to reduce interaction latency, enforce security,\nand encapsulate legacy systems.\"\n\n—Roy Fielding, Architectural Styles and the Design of Network-based Software Architectures\n\nFigure 3-5 shows one of the ways that the taxi-hailing application might use REST.\n\nPOST /trips GET /passengers/<<passengerld>>\n\nPASSENGER ()\nSMARTPHON'!\n\n201 CREATED\n\n200 OK\n\nTRIP\nMANAGEMENT\n\nPASSENGER\nMANAGEMENT\n\nFigure 3-5. A taxi-hailing application uses RESTful interaction.\n\nMicroservices — From Design to Deployment 29 Ch. 3 -Inter-Process Communicatic",
            "status": "success"
        },
        {
            "page_number": 36,
            "text": "The passenger's smartphone requests a trip by making a POST request to the /trips\nresource of the Trip Management service. This service handles the request by sending a\nGET request for information about the passenger to the Passenger Management service.\nAfter verifying that the passenger is authorized to create a trip, the Trip Management\n\nservice creates the trip and returns a 201 response to the smartphone.\n\nMany developers claim their HTTP-based APIs are RESTful. However, as Fielding describes\n\nin this blog post, not all of them actually are.\n\nLeonard Richardson (no relation) defines a very useful maturity model for REST that\n\nconsists of the following levels:\n\nLevel 0 — Clients of a level O API invoke the service by making HTTP Post requests to\nits sole URL endpoint. Each request specifies the action to perform, the target of the\naction (for example, the business object), and any parameters.\n\nLevel 1-A level 1 API supports the idea of resources. To perform an action ona\nresource, a client makes a POST request that specifies the action to perform and\n\nany parameters.\n\nLevel 2—A level 2 API uses HTTP verbs to perform actions: GET to retrieve, POST to\ncreate, and PUT to update. The request query parameters and body, if any, specify\nthe action's parameters. This enables services to leverage web infrastructure such\nas caching for GET requests.\n\nLevel 3 - The design of a level 3 API is based on the terribly named principle, HATEOAS\n(Hypertext As The Engine Of Application State). The basic idea is that the representation\nof a resource returned by a GET request contains links for performing the allowable\nactions on that resource. For example, a client can cancel an order using a link in the\nOrder representation returned in response to the GET request sent to retrieve the order.\n\nOne of the benefits of HATEOAS is include no longer having to hardwire URLs into\nclient code. Another benefit is that because the representation of a resource contains\ninks for the allowable actions, the client doesn't have to guess what actions can be\nperformed on a resource in its current state.\n\nThere are numerous benefits to using a protocol that is based on HTTP:\n\nHTTP is simple and familiar.\n\nYou can test an HTTP API from within a browser using an extension such as Postman,\nor from the command line using curl (assuming JSON or some other text format\nis used).\n\nt directly supports request/response-style communication.\n\nHTTP is, of course, firewall-friendly.\n\nt doesn't require an intermediate broker, which simplifies the system's architecture.\n\nMicroservices — From Design to Deployment 30 1. 3-",
            "status": "success"
        },
        {
            "page_number": 37,
            "text": "There are some drawbacks to using HTTP:\n\n. P only directly supports the request/response style of interaction. You can use\n\nP for notifications but the server must always send an HTTP response.\n\n* Because the client and service communicate directly (without an intermediary to buffer\nmessages), they must both be running for the duration of the exchange.\n\n* The client must know the location (that is, the URL) of each service instance. As described\nin Chapter 2 about the API Gateway, this is a non-trivial problem in a modern application.\nClients must use a service discovery mechanism to locate service instances.\n\nThe developer community has recently rediscovered the value of an interface definition\nlanguage for RESTful APIs. There are a few options, including RAML and Swagger. Some\nIDLs, such as Swagger, allow you to define the format of request and response messages.\nOthers, such as RAML, require you to use a Separate specification such as JSON Schema.\nAs well as describing APIs, IDLs typically have tools that generate client stubs and server\nskeletons from an interface definition.\n\nThrift\n\nApache Thrift is an interesting alternative to REST. It is a framework for writing cross-\nanguage RPC clients and servers. Thrift provides a C-style IDL for defining your APIs.\nYou use the Thrift compiler to generate client-side stubs and server-side skeletons.\nThe compiler generates code for a variety of languages including C++, Java, Python,\nPHP, Ruby, Erlang, and Nodejs.\n\nA Thrift interface consists of one or more services. A service definition is analogous\nto a Java interface. It is a collection of strongly typed methods.\n\nThrift methods can either return a (possibly void) value or, if they are defined as one-way,\nno value. Methods that return a value implement the request/response style of interaction;\nthe client waits for a response and might throw an exception. One-way methods correspond\nto the notification style of interaction; the server does not send a response.\n\nThrift supports various message formats: JSON, binary, and compact binary. Binary is more\nefficient than JSON because it is faster to decode. And, as the name suggests, compact\nbinary is a space-efficient format. JSON is, of course, human- and browser-friendly.\nThrift also gives you a choice of transport protocols including raw TCP and HTTP.\n\nRaw TCP is likely to be more efficient than HTTP. However, HTTP is firewall-friendly,\nbrowser-friendly, and human-friendly.\n\nMessage Formats\n\nNow that we have looked at HTTP and Thrift, let's examine the issue of message formats.\nIf you are using a messaging system or REST, you get to pick your message format.\nOther IPC mechanisms such as Thrift might support only a small number of message\n\nformats, or even just one. In either case, it's important to use a cross-language message\n\nMicroservices — From Design to Deployment 31 1.3-",
            "status": "success"
        },
        {
            "page_number": 38,
            "text": "format. Even if you are writing your microservices in a single language today, it’s likely\nthat you will use other languages in the future.\n\nThere are two main kinds of message formats: text and binary. Examples of text-based\nformats include JSON and XML. An advantage of these formats is that not only are they\nhuman-readable, they are self-describing. In JSON, the attributes of an object are\nrepresented by a collection of name-value pairs. Similarly, in XML the attributes are\nrepresented by named elements and values. This enables a consumer of a message to\npick out the values that it is interested in and ignore the rest. Consequently, minor changes\nto the message format can be easily made backward compatible.\n\nThe structure of XML documents is specified by an XML schema. Over time, the developer\ncommunity has come to realize that JSON also needs a similar mechanism. One option\nis to use JSON Schema, either stand-alone or as part of an IDL such as Swagger.\n\nA downside of using a text-based message format is that the messages tend to be verbose,\nespecially XML. Because the messages are self-describing, every message contains the\nname of the attributes in addition to their values. Another drawback is the overhead of\nparsing text. Consequently, you might want to consider using a binary format.\n\nThere are several binary formats to choose from. If you are using Thrift RPC, you can use\nbinary Thrift. If you get to pick the message format, popular options include Protocol\nBuffers and Apache Avro. Both of these formats provide a typed IDL for defining the\nstructure of your messages. One difference, however, is that Protocol Buffers uses\ntagged fields, whereas an Avro consumer needs to know the schema in order to\ninterpret messages. As a result, API evolution is easier with Protocol Buffers than with\nAvro. This blog post is an excellent comparison of Thrift, Protocol Buffers, and Avro.\n\nSummary\n\nMicroservices must communicate using an inter-process communication mechanism.\nWhen designing how your services will communicate, you need to consider various issues:\nhow services interact, how to specify the API for each service, how to evolve the APIs, and\nhow to handle partial failure. There are two kinds of IPC mechanisms that microservices\ncan use: asynchronous messaging and synchronous request/response. In order to\ncommunicate, one service must be able to find another. In Chapter 4 we will look at the\nproblem of service discovery in a microservices architecture.\n\nMicroservices — From Design to Deployment 32 1. 3-",
            "status": "success"
        },
        {
            "page_number": 39,
            "text": "Microservices in Action: NGINX and Application Architecture\nby Floyd Smith\n\nNGINX enables you to implement various scaling and mirroring options that make your application\nmore responsive and highly available. The choices you make for scaling and mirroring affect how\nyou do inter-process communication, the topic of this chapter.\n\nWe at NGINX recommend that you consider a four-tier architecture when implementing your\nmicroservices-based application. Forrester has a detailed report on the topic which you can down-\nload, at no charge, from NGINX.\n\nThe tiers represent clients (the newest layer — including desktop or laptop and mobile, wearable,\nor loT clients), delivery, aggregation (including data storage), and services, which incorporate\napplication functionality and service-specific, rather than shared, data stores.\n\nThe four-tier architecture is much more flexible, scalable, responsive, mobile-friendly, and inherently\nsupportive of microservices-based application development and delivery than the previous,\nthree-tier architecture. Industry leaders such as Netflix and Uber are able to achieve the level of\nperformance their users demand because they use this kind of architecture.\n\nNGINX is inherently well-suited to the four-tier architecture, with capabilities ranging from media\nstreaming for the client tier, to load balancing and caching for the delivery tier, tools for high-\nperformance and secure API-based communication at the aggregation tier, and support for flexible\nmanagement of ephemeral services instances in the services tier.\n\nThis same flexibility makes it possible to implement robust scaling and mirroring patterns for handling\nchanges in traffic volumes, to protect against security attacks, and to provide high availability with\nfailover configurations available at a moment's notice.\n\nIn these more complex architectures, which include service instance instantiation as demand requires\nand the need for constant service discovery, decoupled inter-process communications tend to\nbe favored. The asynchronous and one-to-many communication styles here may be more flexible,\nand ultimately offer higher performance and reliability, than tightly coupled communication styles.\n\nMicroservices — From Design to Deployment 33",
            "status": "success"
        },
        {
            "page_number": 40,
            "text": "Service\nDiscovery\n\nThis is the fourth chapter in this ebook, which is about building applications with\nmicroservices. Chapter 1 introduces the Microservices Architecture pattern and discussed\nthe benefits and drawbacks of using microservices. Chapter 2 and Chapter 3 describe\ndifferent aspects of communication between microservices. In this chapter, we explore the\nclosely related problem of service discovery.\n\nWhy Use Service Discovery?\n\nLet's imagine that you are writing some code that invokes a service that has a REST API\nor Thrift API. In order to make a request, your code needs to know the network location\n(IP address and port) of a service instance. In a traditional application running on\nphysical hardware, the network locations of service instances are relatively static.\nFor example, your code can read the network locations from a configuration file that\nis occasionally updated.\n\nIn amodern, cloud-based microservices application, however, this is a much more\ndifficult problem to solve, as shown in Figure 4-1.\n\nService instances have dynamically assigned network locations. Moreover, the set\nof service instances changes dynamically because of autoscaling, failures, and\nupgrades. Consequently, your client code needs to use a more elaborate service\ndiscovery mechanism.\n\nMicroservices — From Design to Deployment 34 — Se",
            "status": "success"
        },
        {
            "page_number": 41,
            "text": "SERVICE\nINSTANCE A\n\nSERVICE\nINSTANCE B\n\nRegistry\nClient\n\nFigure 4-1. A client or AP! Gateway needs help finding services.\n\nThere are two main service discovery patterns: client-side discovery and server-side\ndiscovery. Let's first look at client-side discovery.\n\nThe Client-Side Discovery Pattern\n\nWhen using client-side discovery pattern, the client is responsible for determining the\nnetwork locations of available service instances and load balancing requests across them.\nThe client queries a service registry, which is a database of available service instances.\nThe client then uses a load-balancing algorithm to select one of the available service\ninstances and makes a request.\n\nMicroservices — From Design to Deployment 35 Ch. 4 - Service Disc",
            "status": "success"
        },
        {
            "page_number": 42,
            "text": "Figure 4-2 shows the structure of this pattern:\n\nSERVICE\nINSTANCE A\n\nSERVICE\nINSTANCE A| Registry-\naware ‘| Registry\nHTTP : Client\nClient\n\nSERVICE\nINSTANCE B\n\nRegistry\nClient\n\n10.4.3.20:333\n\nRegistry\nClient\n\nFigure 4-2. Clients can take on the task of discovering services.\n\nThe network location of a service instance is registered with the service registry when it\nstarts up. It is removed from the service registry when the instance terminates. The service\ninstance’s registration is typically refreshed periodically using a heartbeat mechanism.\n\nNetflix OSS provides a great example of the client-side discovery pattern. Netflix Eureka\nis a service registry. It provides a REST API for managing service-instance registration and\nfor querying available instances. Netflix Ribbon is an IPC client that works with Eureka to\nload balance requests across the available service instances. We will discuss Eureka in\n\nmore depth later in this chapter.\n\nMicroservices — From Design to Deployment 36 Ch. 4",
            "status": "success"
        },
        {
            "page_number": 43,
            "text": "The client-side discovery pattern has a variety of benefits and drawbacks. This pattern is\nrelatively straightforward and, except for the service registry, there are no other moving\n\nparts.\n\nAlso, since the client knows about the available services instances, it can make\n\nintelligent, application-specific load-balancing decisions such as using hashing consistently.\n\nOne si\nYoum\n\ngnificant drawback of this pattern is that it couples the client with the service registry.\nust implement client-side service discovery logic for each programming language\n\nand framework used by your service clients.\n\nNow that we have looked at client-side discovery, let's take a look at server-side discovery.\n\nThe Server-Side Discovery Pattern\n\nTheo\n\nher approach to service discovery is the server-side discovery pattern. Figure 4-3\n\nshows the structure of this pattern:\n\nSERVICE\nINSTANCE A\n\nREQUEST\n\nRegistry\nClient\n\nREST\nAPI\n\nSERVICE\nINSTANCE B\n\nRegistry\nClient\n\nSERVICE\nINSTANCE A\n\nER 10.4.3.20:333\n\nRegistry\nClient\n\nFigure 4-3. Service discovery can also be handled among servers.\n\nside d\n\nThe client makes a request to a service via a load balancer. The load balancer queries the\nservice registry and routes each request to an available service instance. As with client-\n\niscovery, service instances are registered and deregistered with the service registry.\n\nThe AWS Elastic Load Balancer (ELB) is an example of a server-side discovery router.\n\nELB is commonly used to load balance external traffic from the Internet. However, you\ncan also use ELB to load balance traffic that is internal to a virtual private cloud (VPC).\n\nMicroservices — From Design to Deployment 37 Ch. 4 - Servic",
            "status": "success"
        },
        {
            "page_number": 44,
            "text": "A client makes requests (HTTP or TCP) via the ELB using its DNS name. The ELB load\nbalances the traffic among a set of registered Elastic Compute Cloud (EC2) instances\nor EC2 Container Service (ECS) containers. There isn't a separately visible service\nregistry. Instead, EC2 instances and ECS containers are registered with the ELB itself.\n\nHTTP servers and load balancers such as NGINX Plus and NGINX can also be used as a\nserver-side discovery load balancer. For example, this blog post describes using Consul\nTemplate to dynamically reconfigure NGINX reverse proxying. Consul Template is a tool\nthat periodically regenerates arbitrary configuration files from configuration data stored in\nthe Consul service registry. It runs an arbitrary shell command whenever the files change.\nnthe example described in the blog post, Consul Template generates an nginx.conf\nile, which configures the reverse proxying, and then runs a command that tells NGINX\nto reload the configuration. A more sophisticated implementation could dynamically\nreconfigure NGINX Plus using either its HTTP API or DNS.\n\nSome deployment environments such as Kubernetes and Marathon run a proxy on each\nhost in the cluster. The proxy plays the role of a server-side discovery load balancer. In order\nto make a request to a service, a client routes the request via the proxy using the host's\nIP address and the service's assigned port. The proxy then transparently forwards the\nrequest to an available service instance running somewhere in the cluster.\n\nThe server-side discovery pattern has several benefits and drawbacks. One great benefit\nof this pattern is that details of discovery are abstracted away from the client. Clients simply\nmake requests to the load balancer. This eliminates the need to implement discovery\nlogic for each programming language and framework used by your service clients. Also,\nas mentioned above, some deployment environments provide this functionality for free.\nThis pattern also has some drawbacks, however. Unless the load balancer is provided\nby the deployment environment, it is yet another highly available system component\nthat you need to set up and manage.\n\nThe Service Registry\n\nThe service registry is a key part of service discovery. It is a database containing the\nnetwork locations of service instances. A service registry needs to be highly available\nand up to date. Clients can cache network locations obtained from the service registry.\nHowever, that information eventually becomes out of date and clients become unable\nto discover service instances. Consequently, a service registry consists of a cluster of\nservers that use a replication protocol to maintain consistency.\n\nAs mentioned earlier, Netflix Eureka is good example of a service registry. It provides a\nREST API for registering and querying service instances. A service instance registers its\nnetwork location using a POST request. Every 30 seconds it must refresh its registration\nusing a PUT request. A registration is removed by either using an HTTP DELETE request\nor by the instance registration timing out. As you might expect, a client can retrieve the\nregistered service instances by using an HTTP GET request.\n\nMicroservices — From Design to Deployment 38 -S",
            "status": "success"
        },
        {
            "page_number": 45,
            "text": "Netflix achieves high availability by running one or more Eureka servers in each Amazon\nEC2 availability zone. Each Eureka server runs on an EC2 instance that has an Elastic IP\n\naddress. DNS TEXT records are used to store the Eureka cluster configuration,\n\nwhich is\n\namap from availability zones to a list of the network locations of Eureka servers. When a\n\nEureka server starts up, it queries DNS to retrieve the Eureka cluster configura\nlocates its peers, and assigns itself an unused Elastic IP address.\n\nEureka clients — services and service clients —- query DNS to discover the network\n\nion,\n\nocations\n\nof Eureka servers. Clients prefer to use a Eureka server in the same availability zone.\n\nHowever, if none is available, the client uses a Eureka server in another availabi\n\nOther examples of service registries include:\n* etcd-A highly available, distributed, consistent, key-value store that is used\n\nubernetes and Cloud Foundry.\n\netermine service availability.\n\nistributed applications. Apache ZooKeeper was originally a subproject of H\nbut is now a separate, top-level project.\n\nK\n\n* Consul-—A tool for discovering and configuring services. It provides an API that\na\nd\n\ne Apache Zookeeper — A widely used, high-performance coordination service for\nd\n\nity zone.\n\nor\n\nshared configuration and service discovery. Two notable projects that use etcd are\n\nlows clients to register and discover services. Consul can perform health checks to\n\nadoop,\n\nAlso, as noted previously, some systems such as Kubernetes, Marathon, and AWS do\nnot have an explicit service registry. Instead, the service registry is just a built-in part of\n\nthe infrastructure.\n\nNow that we have looked at the concept of a service registry, let's look at how service\n\ninstances are registered with the service registry.\n\nService Registration Options\n\nAs previously mentioned, service instances must be registered with and unregistered from\nthe service registry. There are a couple of different ways to handle the registration and\n\nunregistration. One option is for service instances to register themselves, th\n\ne self-\n\nregistration pattern. The other option is for some other system component to manage\nthe registration of service instances, the third-party registration pattern. Let's first look\n\nat the self-registration pattern.\n\nThe Self-Registration Pattern\n\nWhen using the self-registration pattern, a service instance is responsible for re\n\ngistering\n\nand unregistering itself with the service registry. Also, if required, a service instance sends\n\nheartbeat requests to prevent its registration from expiring.\n\nMicroservices — From Design to Deployment 39",
            "status": "success"
        },
        {
            "page_number": 46,
            "text": "Figure 4-4 shows the structure of this pattern.\n\n10.4.3.1:8756\n\nSERVICE\nINSTANCE A\n\nregister(\"serviceName, \"10.4.3.1:8756\")\nheartbeat()\nunregister()\n\nFigure 4-4. Services can handle their own registration.\n\nA good example of this approach is the Netflix OSS Eureka client. The Eureka client handles\nall aspects of service instance registration and unregistration. The Spring Cloud project,\nwhich implements various patterns including service discovery, makes it easy to\nautomatically register a service instance with Eureka. You simply annotate your Java\nConfiguration class with an @EnableEurekaClient annotation.\n\nThe self-registration pattern has various benefits and drawbacks. One benefit is that it\nis relatively simple and doesn't require any other system components. However, a major\ndrawback is that it couples the service instances to the service registry. You must\nimplement the registration code in each programming language and framework used\nby your services.\n\nThe alternative approach, which decouples services from the service registry, is the\nthird-party registration pattern.\n\nMicroservices — From Design to Deployment 40 Ch. 4 - Service Discovery",
            "status": "success"
        },
        {
            "page_number": 47,
            "text": "The Third-Party Registration Pattern\n\nWhen using the third-party registration pattern, service instances aren't responsible for\nregistering themselves with the service registry. Instead, another system component\nknown as the service registrar handles the registration. The service registrar tracks\nchanges to the set of running instances by either polling the deployment environment\nor subscribing to events. When it notices a newly available service instance, it registers\nthe instance with the service registry. The service registrar also unregisters terminated\nservice instances.\n\nFigure 4-5 shows the structure of this pattern:\n\n10.4.3.1:8756 HEALTHCHECK\n\nSERVICE\nINSTANCE A\n\nregister(\"serviceName, \"10.4.3.1:8756\")\nheartbeat()\nunregister()\n\nFigure 4-5. A separate registrar service can be responsible for registering others.\n\nOne example of a service registrar is the open source Registrator project. It automatically\nregisters and unregisters service instances that are deployed as Docker containers.\nRegistrator supports several service registries, including etcd and Consul.\n\nAnother example of a service registrar is NetflixOSS Prana. Primarily intended for services\nwritten in non-JVM languages, it is a sidecar application that runs side by side with a\nservice instance. Prana registers and unregisters the service instance with Netflix Eureka.\n\nThe service registrar is a built-in Component in some deployment environments. The EC2\ninstances created by an Autoscaling Group can be automatically registered with an ELB.\nKubernetes services are automatically registered and made available for discovery.\n\nMicroservices — From Design to Deployment 41 Ch. 4 - Service Discovery",
            "status": "success"
        },
        {
            "page_number": 48,
            "text": "The third-party registration pattern has various benefits and drawbacks. A major benefit\nis that services are decoupled from the service registry. You don't need to implement\nservice-registration logic for each programming language and framework used by your\ndevelopers. Instead, service instance registration is handled in a centralized manner\nwithin a dedicated service.\n\nOne drawback of this pattern is that unless it's built into the deployment environment,\nitis yet another highly available system component that you need to set up and manage.\n\nSummary\n\nIn a microservices application, the set of running service instances changes dynamically.\nInstances have dynamically assigned network locations. Consequently, in order for a client\nto make a request to a service it must use a Service-discovery mechanism.\n\nAkey part of service discovery is the service registry. The service registry is a database\nof available service instances. The service registry provides a management API anda\nquery API. Service instances are registered with and unregistered from the service registry\nusing the management API. The query API is used by system components to discover\navailable service instances.\n\nThere are two main service-discovery patterns: client-side discovery and service-side\ndiscovery. In systems that use client-side service discovery, clients query the service\nregistry, select an available instance, and make a request. In systems that use server-\nside discovery, clients make requests via a router, which queries the service registry\nand forwards the request to an available instance.\n\nThere are two main ways that service instances are registered with and unregistered from\nthe service registry. One option is for service instances to register themselves with the\nservice registry, the self-registration pattern. The other option is for some other system\ncomponent to handle the registration and unregistration on behalf of the service,\nthe third-party registration pattern.\n\nIn some deployment environments you need to set up your own service-discovery\ninfrastructure using a service registry such as Netflix Eureka, etcd, or Apache Zookeeper.\nIn other deployment environments, service discovery is built in. For example, Kubernetes\nand Marathon handle service instance registration and unregistration. They also runa\nproxy on each cluster host that plays the role of server-side discovery router.\n\nAn HTTP reverse proxy and load balancer such as NGINX can also be used as a Server-\nside discovery load balancer. The service registry can push the routing information to\nNGINX and invoke a graceful configuration update; for example, you can use Consul\nTemplate. NGINX Plus supports additional dynamic reconfiguration mechanisms — it can\npull information about service instances from the registry using DNS, and it provides an\nAPI for remote reconfiguration.\n\nMicroservices — From Design to Deployment 42 —Se",
            "status": "success"
        },
        {
            "page_number": 49,
            "text": "Microservices in Action: NGINX Flexibility\nby Floyd Smith\n\nIn a microservices environment, your backend infrastructure is likely to be constantly changing\nas services are created, deployed, and scaled up and down as a result of autoscaling, failures, and\nupgrades. As described in this chapter, a service discovery mechanism is required in environ-\nments where service locations are dynamically reassigned.\n\nPart of the benefit of using NGINX for microservices is that you can easily configure it to automat-\nically react to changes in backend infrastructure. NGINX configuration is not only easy and flexible,\nit's also compatible with the use of templates, as used in Amazon Web Services, making it easier\nto manage changes for a specific service and to manage changing sets of services subject to\nload balancing.\n\nNGINX Plus features an on-the-fly reconfiguration API, eliminating the need to restart NGINX Plus\nor manually reload its configuration to get it to recognize changes to the set of services being load\nbalanced. In NGINX Plus Release 8 and later, the changes you make with the API can be configured\nto persist across restarts and configuration reloads. (Reloads do not require a restart and do not\ndrop connections.) And NGINX Plus Release 9 and later have support for service discovery using\nDNS sRvV records, enabling tighter integration with existing server discovery platforms, such as\nConsul and etcd.\n\nWe here at NGINX have created a model for managing service discovery:\n\n1. Run separate Docker containers for each of several apps, including a service discovery app\nsuch as etcd, a service registration tool, one or more backend servers, and NGINX Plus itself to\nload balance the other containers.\n\n2. The registration tool monitors Docker for new containers and registers new services with the\nservice discovery tool, also removing containers that disappear.\n\n3. Containers and the services they run are automatically added to or removed from the group of\nload-balanced upstream servers.\n\nDemo apps for this process are available for several service-discovery apps: Consul APIs, DNS\nSRV records from Consul, etcd, and ZooKeeper.\n\nMicroservices — From Design to Deployment 43",
            "status": "success"
        },
        {
            "page_number": 50,
            "text": "Event-Driven Data\nManagement for\nMicroservices\n\n[his is the fifth chapter of this ebook about building applications with microservices.\nThe first chapter introduces the Microservices Architecture pattern and discusses the\nbenefits and drawbacks of using microservices. The second and third describe different\naspects of communication within a microservices architecture. The fourth chapter explores\nthe closely related problem of service discovery. In this chapter, we change gears and look\nat the distributed data management problems that arise in a microservices architecture.\n\nMicroservices and the Problem of Distributed Data Management\n\nA monolithic application typically has a single relational database. A key benefit of using\na relational database is that your application can use ACID transactions, which provide\nsome important guarantees:\n\n° Atomicity - Changes are made atomically\n\n* Consistency — The state of the database is always consistent\n\n¢ Isolation — Even though transactions are executed concurrently, it appears they\nare executed serially\n\n¢ Durable - Once a transaction has committed, it is not undone\n\nAs aresult, your application can simply begin a transaction, change (insert, update,\nand delete) multiple rows, and commit the transaction.\n\nMicroservices — From Design to Deployment 44 Ch. 5 -Event-D Dat",
            "status": "success"
        },
        {
            "page_number": 51,
            "text": "Another great benefit of using a relational database is that it provides SQL, which is a\nrich, declarative, and standardized query language. You can easily write a query that\ncombines data from multiple tables. The RDBMS query planner then determines the\noptimal way to execute the query. You don't have to worry about low-level details such\nas how to access the database. And, because all of your application's data is in one\ndatabase, it is easy to query.\n\nUnfortunately, data access becomes much more complex when we move to a\nmicroservices architecture. That is because the data owned by each microservice\n\nis private to that microservice and can only be accessed via its API. Encapsulating the\ndata ensures that the microservices are loosely coupled and can evolve independently\nof one another. If multiple services access the same data, schema updates require\ntime-consuming, coordinated updates to all of the services.\n\nTo make matters worse, different microservices often use different kinds of databases.\nodern applications store and process diverse kinds of data, and a relational database\nis not always the best choice. For some use cases, a particular NoSQL database might\nhave a more convenient data model and offer much better performance and scalability.\nFor example, it makes sense for a service that stores and queries text to use a text search\nengine such as Elasticsearch. Similarly, a service that stores social graph data should\nprobably use a graph database, such as Neo4j. Consequently, microservices-based\napplications often use a mixture of SQL and NoSQL databases, the so-called polyglot\npersistence approach.\n\nA partitioned, polyglot-persistent architecture for data storage has many benefits, including\nloosely coupled services and better performance and scalability. However, it does introduce\nsome distributed data management challenges.\n\nThe first challenge is how to implement business transactions that maintain consistency\nacross multiple services. To see why this is a problem, let's take a look at an example of\nan online B2B store. The Customer Service maintains information about customers,\nincluding their credit lines. The Order Service manages orders and must verify that a\nnew order doesn't violate the customer's credit limit. In the monolithic version of this\napplication, the Order Service can simply use an ACID transaction to check the available\ncredit and create the order.\n\nMicroservices — From Design to Deployment 45 Ch. 5 — Event-Driven Di anac",
            "status": "success"
        },
        {
            "page_number": 52,
            "text": "In contrast, ina microservices architecture the ORDER and CUSTOMER tables are private\nto their respective services, as shown in Figure 5-1:\n\nORDER table\n\niD CUSTOMER ID STATUS\n\nFigure 5-1. Microservices each have the.\n\nTOTAL\n\nir own data.\n\nCUSTOMER table\n\niD\n\nCREDIT LIMIT\n\nThe Order Service cannot access the CUSTOMER table directly. It can only use the API\nprovided by the Customer Service. The Order Service could potentially use distributed\ntransactions, also known as two-phase commit (2PC). However, 2PC is usually not a viable\noption in modern applications. The CAP theorem requires you to choose between\navailability and ACID-style consistency, and availability is usually the better choice.\n\nMoreover, many modern techno\nsupport 2PC. Maintaining data con\nso we need another solution.\n\nThe second challenge is how to\n\nogies, such as mos\nsistency across serv\n\nNoSQL databases, do not\nices and databases is essential,\n\nimplement queries that retrieve data from multiple\n\nservices. For example, let's imagine that the application needs to display a customer and\n\nhis recent orders. If the Order Ser\nthen you can retrieve this data usi\nthe customer from the Customer\nService. Suppose, however, that t\n\nvice provides an API\n\nService and the cus\nhe Order Service on\n\nby their primary key (perhaps it u\n\nses a NoSQL datab\n\nkey-based retrievals). In this situation, there is no obviou\n\nfor retrieving a customer's orders\n\nng an application-side join. The application retrieves\n\ntomer's orders from the Order\n\ny supports the lookup of orders\n\nase that only supports primary\n\ns way to retrieve the needed data.\n\nMicroservices — From Design to Deployment\n\n46",
            "status": "success"
        },
        {
            "page_number": 53,
            "text": "Event-Driven Architecture\n\nFor many applications, the solution is to use an event-driven architecture. In this\narchitecture, a microservice publishes an event when something notable happens,\nsuch as when it updates a business entity. Other microservices subscribe to those\nevents. When a microservice receives an event it can update its own business entities,\nwhich might lead to more events being published.\n\nYou can use events to implement business transactions that span multiple services.\n\nA transaction consists of a series of steps. Each step consists of a microservice updating\na business entity and publishing an event that triggers the next step. The following\nsequence of diagrams shows how you can use an event-driven approach to checking\nfor available credit when creating an order.\n\nThe microservices exchange events via a Message Broker:\n\n¢ The Order Service creates an Order with status NEW and publishes an Order\nCreated event.\n\nOrder\n- y—\n\nORDER table\n\na CUSTOMER table\n\n1 CREDIT_uMIT\n\n10 CUSTOMER © status TOTAL\n\n202 S000\n\nRESERVED CREDIT table\n\nCUSTID ORDERID AMOUNT\n\nFigure 5-2. The Order Service publishes an event.\n\nMicroservices — From Design to Deployment 47 Ch. 5 —Event-Driven Data Management for M",
            "status": "success"
        },
        {
            "page_number": 54,
            "text": "¢ The Customer Service consumes the Order Created event, reserves credit for the\norder, and publishes a Credit Reserved event.\n\n1D CUSTOMER D> ‘STATUS\n\nse 101 New\n\nRESERVED CREDIT table\n\nFigure 5-3. The Customer Service responds.\n\n¢ The Order Service consumes the Credit Reserved event and changes the status of\nthe order to OPEN.\n\nORDER table ; (won r CUSTOMER table\n\n> CUSTOMER_ID status TOTAL\n\nCREDIT. Liner\n\n908 101 OPEN 1234\n\nRESERVED CREDIT table\n\nCUSTID © ORDERID AMOUNT\n\nFigure 5-4. The Order Service acts on the response.\n\nA more complex scenario could involve additional steps, such as reserving inventory at\nthe same time as the customer's credit is checked.\n\nMicroservices — From Design to Deployment 48 Ch. 5 —-Event-Driven Data Management for Micro",
            "status": "success"
        },
        {
            "page_number": 55,
            "text": "Provided that (a) each service atomically updates the database and publishes an event\n—more on that later — and (b) the Message Broker guarantees that events are delivered\nat least once, then you can implement business transactions that span multiple services.\nIt is important to note that these are not ACID transactions. They offer much weaker\nguarantees such as eventual consistency. This transaction model has been referred to\nas the BASE model.\n\nYou can also use events to maintain materialized views that pre-join data owned by\nmultiple microservices. The service that maintains the view subscribes to the relevant\nevents and updates the view. Figure 5-5 depicts a Customer Order View Updater Service\nthat updates the Customer Order View based on events published by the Customer\nService and Order Service.\n\n[= J [a= | CUSTOMER ORDER,\nVIEW UPDATER\n\nOrder Created Customer Created\n\nOrder Cancetied Customer Cancelled\n\nOrder Shipped Customer Shipped\n\nFigure 5-5. The Customer Order View is accessed by two services.\n\nWhen the Customer Order View Updater Service receives a Customer or Order event, it\nupdates the Customer Order View datastore. You could implement the Customer Order\nView using a document database such as MongoDB and store one document for each\nCustomer. The Customer Order View Query Service handles requests for a customer\nand recent orders by querying the Customer Order View datastore.\n\nAn event-driven architecture has several benefits and drawbacks. It enables the\nimplementation of transactions that span multiple services and provide eventual\nconsistency. Another benefit is that it also enables an application to maintain\nmaterialized views.\n\nMicroservices — From Design to Deployment 49 Ch. 5 - Event-Dri Data Management for M",
            "status": "success"
        },
        {
            "page_number": 56,
            "text": "One drawback is that the programming model is more complex than when using ACID\ntransactions. Often you must implement compensating transactions to recover from\napplication-level failures; for example, you must cancel an order if the credit check fails.\nAlso, applications must deal with inconsistent data. That is because changes made by\nin-flight transactions are visible. The application can also see inconsistencies if it reads\nfrom a materialized view that is not yet updated. Another drawback is that subscribers\nmust detect and ignore duplicate events.\n\nAchieving Atomicity\n\nIn an event-driven architecture there is also the problem of atomically updating the\ndatabase and publishing an event. For example, the Order Service must insert a row\ninto the ORDER table and publish an Order Created event. It is essential that these two\noperations are done atomically. If the service crashes after updating the database but\nbefore publishing the event, the system becomes inconsistent. The standard way to\nensure atomicity is to use a distributed transaction involving the database and the\nMessage Broker. However, for the reasons described above, such as the CAP theorem,\nthis is exactly what we do not want to do.\n\nPublishing Events Using Local Transactions\n\nOne way to achieve atomicity is for the application to publish events using a multi-step\nprocess involving only local transactions. The trick is to have an EVENT table, which\nfunctions as a message queue, in the database that stores the state of the business\nentities. The application begins a (local) database transaction, updates the state of the\nbusiness entities, inserts an event into the EVENT table, and commits the transaction.\nA separate application thread or process queries the EVENT table, publishes the events\nto the Message Broker, and then uses a local transaction to mark the events as published.\nFigure 5-6 shows the design.\n\nPublish\nORDER\nf\nLocal Transaction\nINSERT INSERT QUERY\nMi\nORDER table EVENT table many\n\n10 «CUSTOMER ID sTaTUS TOTAL\n\n909 101 new 1264\n\nFigure 5-6. Achieving atomicity with local transactions.\n\nMicroservices — From Design to Deployment 50 Ch. 5 - Event-Dri Data Management for M",
            "status": "success"
        },
        {
            "page_number": 57,
            "text": "The Order Service inserts a row into the ORDER table and inserts an Order Created event\ninto the EVENT table. The Event Publisher thread or process queries the EVENT table for\nunpublished events, publishes the events, and then updates the EVENT table to mark\n\nthe events as published.\n\nThis approach has several benefits and drawbacks. One benefit is that it guarantees an\nevent is published for each update without relying on 2PC. Also, the application publishes\nbusiness-level events, which eliminates the need to infer them. One drawback of this\napproach is that it is potentially error-prone since the developer must remember to publish\nevents. A limitation of this approach is that it is challenging to implement when using\nsome NoSQL databases because of their limited transaction and query capabilities.\n\nThis approach eliminates the need for 2PC by having the application use local transactions\nto update state and publish events. Let's now look at an approach that achieves atomicity\nby having the application simply update state.\n\nMining a Database Transaction Log\n\nAnother way to achieve atomicity without 2PC is for the events to be published by a thread\nor process that mines the database's transaction or commit log. The application updates\nthe database, so changes are recorded in the database's transaction log. The Transaction\nLog Miner thread or process reads the transaction log and publishes events to the\nMessage Broker. Figure 5-7 shows the design.\n\nDatastore\n\nORDER table\n\nMESSAGE\nBROKER\n\nFigure 5-7. A Message Broker can arbitrate data transactions.\n\nMicroservices — From Design to Deployment 51 Ch. 5 — Event-Driven Data Management for M",
            "status": "success"
        },
        {
            "page_number": 58,
            "text": "An example of this approach is the open source Linkedin Databus project. Databus mines\nthe Oracle transaction log and publishes events corresponding to the changes. LinkedIn\nuses Databus to keep various derived data stores consistent with the system of record.\n\nAnother example is the streams mechanism in AWS DynamoDB, which is a managed\nNoSQL database. A DynamoDB stream contains the time-ordered sequence of changes\n(create, update, and delete operations) made to the items ina DynamoDB table in the\nlast 24 hours. An application can read those changes from the stream and, for example,\npublish them as events.\n\nTransaction log mining has various benefits and drawbacks. One benefit is that it\nguarantees that an event is published for each update without using 2PC. Transaction\nlog mining can also simplify the application by separating event publishing from the\napplication's business logic. A major drawback is that the format of the transaction log\nis proprietary to each database and can even change between database versions.\nAlso, it can be difficult to reverse engineer the high-level business events from the\nlow-level updates recorded in the transaction log.\n\nTransaction log mining eliminates the need for 2PC by having the application do one\nthing: update the database. Let's now look at a different approach that eliminates the\nupdates and relies solely on events.\n\nUsing Event Sourcing\n\nEvent sourcing achieves atomicity without 2PC by using a radically different, event-\ncentric approach to persisting business entities. Rather than store the current state of\nan entity, the application stores a sequence of state-changing events. The application\nreconstructs an entity's current state by replaying the events. Whenever the state of a\nbusiness entity changes, a new event is appended to the list of events. Since saving an\nevent is a single operation, it is inherently atomic.\n\nTo see how event sourcing works, consider the Order entity as an example. In a traditional\napproach, each order maps to a row in an ORDER table and to rows in, for example,\nan ORDER_LINE_ITEM table.\n\nBut when using event sourcing, the Order Service stores an Order in the form of its\nstate-changing events: Created, Approved, Shipped, Cancelled. Each event contains\nsufficient data to reconstruct the Order's state.\n\nMicroservices — From Design to Deployment 52 Ch.5-E -Dri D anac",
            "status": "success"
        },
        {
            "page_number": 59,
            "text": "eS ‘sl 1S)\n\n‘Add Events , ‘Subscribe to Events\nFind Events |\n\nFigure 5-8. Events can have complete recovery data.\n\nEvents persist in an Event Store, which is a database of events. The store has an API for\nadding and retrieving an entity's events. The Event Store also behaves like the Message\nBroker in the architectures we described previously. It provides an API that enables services\nto subscribe to events. The Event Store delivers all events to all interested subscribers.\nThe Event Store is the backbone of an event-driven microservices architecture.\n\nEvent sourcing has several benefits. It solves one of the key problems in implementing an\nevent-driven architecture and makes it possible to reliably publish events whenever state\nchanges. As a result, it solves data consistency issues in a microservices architecture.\nAlso, because it persists events rather than domain objects, it mostly avoids the object-\nrelational impedance mismatch problem. Event sourcing also provides a 100% reliable\naudit log of the changes made to a business entity and makes it possible to implement\ntemporal queries that determine the state of an entity at any point in time. Another major\nbenefit of event sourcing is that your business logic consists of loosely coupled business\nentities that exchange events. This makes it a lot easier to migrate from a monolithic\napplication to a microservices architecture.\n\nEvent sourcing also has some drawbacks. It is a different and unfamiliar style of\nprogramming and so there is a learning curve. The event store only directly supports\nthe lookup of business entities by primary key. You must use command query\nresponsibility separation (CQRS) to implement queries. As a result, applications\nmust handle eventually consistent data.\n\nMicroservices — From Design to Deployment 53 Ch. 5 - Event-Dri Data Management for M",
            "status": "success"
        },
        {
            "page_number": 60,
            "text": "Summary\n\nIn a microservices architecture, each microservice has its own private datastore. Different\nmicroservices might use different SQL and NoSQL databases. While this database\narchitecture has significant benefits, it creates some distributed data management\nchallenges. The first challenge is how to implement business transactions that maintain\nconsistency across multiple services. The second challenge is how to implement queries\nthat retrieve data from multiple services.\n\nFor many applications, the solution is to use an event-driven architecture. One challenge\nwith implementing an event-driven architecture is how to atomically update state and\nhow to publish events. There are a few ways to accomplish this, including using the\ndatabase as a message queue, transaction log mining, and event sourcing.\n\nMicroservices in Action: NGINX and Storage Optimization\nby Floyd Smith\n\nAmicroservices-based approach to storage involves a greater number and variety of data stores,\nmore complexity in how you access and update data, and greater challenges for both Dev and Ops\nin maintaining data consistency. NGINX provides crucial support for this kind of data management,\nin three main areas:\n\n=\n\n. Caching and microcaching of data — Caching static files and microcaching application-gen-\nerated content with NGINX reduces the load on your application, increasing performance and\nreducing the potential for problems.\n\nN\n\nFlexibility and scalability per data store —- Once you implement NGINX as a reverse proxy\nserver, your apps gain great flexibility in creating, sizing, running, and resizing data storage\nservers to meet changing requirements — vital when every service has its own data store.\n\nWw\n\n. Monitoring and management of services, including data services — With the number of data\nservers multiplying, supporting complex operations is critical, as are monitoring and management\ntools. NGINX Plus has built-in tools and interfaces to application performance management\npartners such as Data Dog, Dynatrace, and New Relic.\n\nExamples of microservice-specific data management are included in the three Models of the\nNGINX Microservices Reference Architecture, giving you a starting point for your own design\ndecisions and implementation.\n\nMicroservices — From Design to Deployment 54 ( ri }",
            "status": "success"
        },
        {
            "page_number": 61,
            "text": "Choosing a\nMicroservices\nDeployment\nstrategy\n\nThis is the sixth chapter in this ebook about building applications with microservices.\nChapter 1 introduces the Microservices Architecture pattern and discusses the benefits\nand drawbacks of using microservices. The following chapters discuss different aspects\nof the microservices architecture: using an API Gateway, inter-process communication,\nservice discovery, and event-driven data management. In this chapter, we look at strategies\nfor deploying microservices.\n\nMotivations\n\nDeploying a monolithic a\nsingle, usually large, appl\n\npplication means running one or more identical copies of a\nication. You typically provision N servers (physical or virtual)\n\nand run M instances of the application on each server. The deployment of a monolithic\n\napplication is not always\n\na microservices applicati\n\nA microservices application consists of tens or even hundreds o\n\nentirely straightforward, but it is much si\non.\n\nmpler than deploying\n\nservices. Services are\n\nwritten in a variety of languages and frameworks. Each one is a mini-application with its\n\nown specific deploymen\n\nyou need to run a certain\n\nthat service. Also, each service instance must be provided with\n\nmemory, and I/O resourc\n\n, resource, scaling, and monitoring requi\nnumber of instances of each service based on the demand for\n\nrements. For example,\n\nthe appropriate CPU,\n\nes. What is even more challenging is tha\n\ndespite this\n\ncomplexity, deploying services must be fast, reliable and cost-effective.\n\nMicroservices — From Design to Deploymen",
            "status": "success"
        },
        {
            "page_number": 62,
            "text": "There are a few different microservice deployment patterns. Let's look first at the\nMultiple Service Instances per Host pattern.\n\nMultiple Service Instances Per Host Pattern\n\nOne way to deploy your microservices is to use the Multiple Service Instances per Host\npattern. When using this pattern, you provision one or more physical or virtual hosts and\nrun multiple service instances on each one. In many ways, this is the traditional approach\nto application deployment. Each service instance runs at a well-known port on one or\nmore hosts. The host machines are commonly treated like pets.\n\nFigure 6-1 shows the structure of this pattern:\n\nHost (Physical or VM)\n\nSERVICE-A SERVICE-B\nINSTANCE-1 INSTANCE-1\n\nHost (Physical or VM)\n\nSERVICE-A SERVICE-B\nINSTANCE-2 INSTANCE-2\n\nFigure 6-1. Hosts can each support multiple service instances.\n\nMicroservices — From Design to Deployment 56\n\nloyment Strategy",
            "status": "success"
        },
        {
            "page_number": 63,
            "text": "There are a couple of variants of this pattern. One variant is for each service instance to\nbe a process or a process group. For example, you might deploy a Java service instance\nas a web application on an Apache Tomcat server. A Node,js service instance might\nconsist of a parent process and one or more child processes.\n\nThe other variant of this pattern is to run multiple service instances in the same process\nor process group. For example, you could deploy multiple Java web applications on the\nsame Apache Tomcat server or run multiple OSGI bundles in the same OSGI container.\n\nThe Multiple Service Instances per Host pattern has both benefits and drawbacks.\nOne major benefit is its resource usage is relatively efficient. Multiple service instances\nshare the server and its operating system. It's even more efficient if a process or group\nruns multiple service instances, for example, multiple web applications sharing the same\nApache Tomcat server and JVM.\n\nAnother benefit of this pattern is that deploying a service instance is relatively fast.\nYou simply copy the service to a host and start it. If the service is written in Java, you copy\na JAR or WAR file. For other languages, such as Node.js or Ruby, you copy the source\ncode. In either case, the number of bytes copied over the network is relatively small.\n\nAlso, because of the lack of overhead, starting a service is usually very fast. If the service\nis its own process, you simply start it. Otherwise, if the service is one of several instances\nrunning in the same container process or process group, you either dynamically deploy\nit into the container or restart the container.\n\nDespite its appeal, the Multiple Service Instances per Host pattern has some significant\ndrawbacks. One major drawback is that there is little or no isolation of the service\ninstances, unless each service instance is a separate process. While you can accurately\nmonitor each service instance's resource utilization, you cannot limit the resources each\ninstance uses. It's possible for a misbehaving service instance to consume all of the\nmemory or CPU of the host.\n\nThere is no isolation at all if multiple service instances run in the same process.\nAll instances might, for example, share the same JVM heap. A misbehaving service\ninstance could easily break the other services running in the same process. Moreover,\nyou have no way to monitor the resources used by each service instance.\n\nAnother significant problem with this approach is that the operations team that deploys a\nservice has to know the specific details of how to do it. Services can be written in a variety\nof languages and frameworks, so there are lots of details that the development team must\nshare with operations. This complexity increases the risk of errors during deployment.\n\nAs you can see, despite its familiarity, the Multiple Service Instances per Host pattern has\nsome significant drawbacks. Let's now look at other ways of deploying microservices\nthat avoid these problems.\n\nMicroservices — From Design to Deployment 57 Ch. 6- Choosing aM vices Deplc Str",
            "status": "success"
        },
        {
            "page_number": 64,
            "text": "Service Instance per Host Pattern\n\nAnother way to deploy your microservices is the Service Instance per Host pattern.\nWhen you use this pattern, you run each service instance in isolation on its own host.\nThere are two different different specializations of this pattern: Service Instance per\nVirtual Machine and Service Instance per Container.\n\nService Instance per Virtual Machine Pattern\n\nWhen you use Service Instance per Virtual Machine pattern, you package each service\nas a virtual machine (VM) image such as an Amazon EC2 AMI. Each service instance is\naVM (for example, an EC2 instance) that is launched using that VM image.\n\nFigure 6-2 shows the structure of this pattern:\n\nVM\nDeployed As\nVM\nPackaged As q\n—————_ ; VM Image :ofomoHe—>\nVM\n\nFigure 6-2. Services can each live in their own virtual machine.\n\nMicroservices — From Design to Deployment 58 Ch. 6 — Choosing a Microservices Deployment Strategy",
            "status": "success"
        },
        {
            "page_number": 65,
            "text": "This is the primary approach used by Netflix to deploy its video streaming service.\nNetflix packages each of its services as an EC2 AMI using Aminator. Each running service\ninstance is an EC2 instance.\n\nThere are a variety tools that you can use to build your own VMs. You can configure your\ncontinuous integration (Cl) server (for example, Jenkins) to invoke Aminator to package\nyour services as an EC2 AMI. Packer is another option for automated VM image creation.\nUnlike Aminator, it supports a variety of virtualization technologies including EC2,\nDigitalOcean, VirtualBox, and VMware.\n\nThe company Boxfuse has a compelling way to build VM images, which overcomes the\ndrawbacks of VMs that | describe below. Boxfuse packages your Java application as a\nSs\n\nnimal VM image. These images are fast to build, boot quickly, and are more secure\nnce they expose a limited attack surface.\n\nhe company CloudNative has the Bakery, a SaaS offering for creating EC2 AMls. You can\nconfigure your Cl server to invoke the Bakery after the tests for your microservice pass.\nThe Bakery then packages your service as an AMI. Using a SaaS offering such as the Bakery\nmeans that you don't have to waste valuable time setting up the AMI creation infrastructure.\n\nhe Service Instance per Virtual Machine pattern has a number of benefits. A major benefit\nof VMs is that each service instance runs in complete isolation. It has a fixed amount of\nCPU and memory and can't steal resources from other services.\n\nAnother benefit of deploying your microservices as VMs is that you can leverage mature\ncloud infrastructure. Clouds such as AWS provide useful features such as load balancing\nand autoscaling.\n\nAnother great benefit of deploying your service as a VM is that it encapsulates your\nservice's implementation technology. Once a service has been packaged as a VM it\nbecomes a black box. The VM's management API becomes the API for deploying the\nservice. Deployment becomes much simpler and more reliable.\n\nThe Service Instance per Virtual Machine pattern has some drawbacks, however.\nOne drawback is less efficient resource utilization. Each service instance has the overhead\nof an entire VM, including the operating system. Moreover, in a typical public laaS, VMs\ncome in fixed sizes and it is possible that the VM will be underutilized.\n\noreover, a public laaS typically charges for VMs regardless of whether they are busy\n\nor idle. An laaS such as AWS provides autoscaling, but it is difficult to react quickly to\nchanges in demand. Consequently, you often have to overprovision VMs, which increases\nhe cost of deployment.\n\nAnother downside of this approach is that deploying a new version of a service is usually\nslow. VM images are typically slow to build due to their size. Also, VMs are typically slow\no instantiate, again because of their size. Also, an operating system typically takes some\nime to start up. Note, however, that this is not universally true, since lightweight VMs such\nas those built by Boxfuse exist.\n\nMicroservices — From Design to Deployment 59 Ch. 6 — Choosing a Mic",
            "status": "success"
        },
        {
            "page_number": 66,
            "text": "Another drawback of the Service Instance per Virtual Machine pattern is that usually you\n(or someone else in your organization) are responsible for a lot of undifferentiated heavy\nlifting. Unless you use a tool such as Boxfuse that handles the overhead of building and\nmanaging the VMs, then it is your responsibility. This necessary but time-consuming\nactivity distracts from your core business.\n\nLet's now look at an alternative way to deploy microservices that is more lightweight,\nyet still has many of the benefits of VMs.\n\nService Instance per Container Pattern\n\nWhen you use the Service Instance per Container pattern, each service instance runs in\nits own container. Containers are a virtualization mechanism at the operating system level.\nAcontainer consists of one or more processes running in a sandbox. From the perspective\nof the processes, they have their own port namespace and root filesystem. You can limit\nacontainer's memory and CPU resources. Some container implementations also have\nI/O rate limiting. Examples of container technologies include Docker and Solaris Zones.\n\nFigure 6-3 shows the structure of this pattern:\n\nVM\nContainer\nDeployed As\nVM\na Container\nPackagedAs :\nR q Container -\nq Image }\nVM\nContainer\n\nFigure 6-3. Services can each live in their own container.\n\nMicroservices — From Design to Deployment 60 Ch. 6 — Choosing a Microservices Deployment Strategy",
            "status": "success"
        },
        {
            "page_number": 67,
            "text": "To use this pattern, you package your service as a container image. A container image is\na filesystem image consisting of the applications and libraries required to run the service.\nSome container images consist of a complete Linux root filesystem. Others are more\nlightweight. To deploy a Java service, for example, you build a container image containing\nthe Java runtime, perhaps an Apache Tomcat server, and your compiled Java application.\n\nOnce you have packaged your service as a container image, you then launch one or\nmore containers. You usually run multiple containers on each physical or virtual host.\nYou might use a cluster manager such as Kubernetes or Marathon to manage your\ncontainers. A cluster manager treats the hosts as a pool of resources. It decides\nwhere to place each container based on the resources required by the container and\nresources available on each host.\n\nThe Service Instance per Container pattern has both benefits and drawbacks. The benefits\n\nof containers are similar to those of VMs. They isolate your service instances from each\nother. You can easily monitor the resources consumed by each container. Also, like VMs,\n\ncontainers encapsulate the technology used to implement your services. The container\nmanagement API also serves as the API for managing your services.\n\nHowever, unlike VMs, containers are a lightweight technology. Container images are\ntypically very fast to build. For example, on my laptop it takes as little as 5 seconds to\npackage a Spring Boot application as a Docker container. Containers also start very quickly,\nsince there is no lengthy OS boot mechanism. When a container starts, what runs is\nthe service.\n\nThere are some drawbacks to using containers. While container infrastructure is rapidly\nmaturing, it is not as mature as the infrastructure for VMs. Also, containers are not as\nsecure as VMs, since the containers share the kernel of the host OS with one another.\n\nAnother drawback of containers is that you are responsible for the undifferentiated\nheavy lifting of administering the container images. Also, unless you are using a hosted\ncontainer solution such as Google Container Engine or Amazon EC2 Container Service\n(ECS), then you must administer the container infrastructure and possibly the VM\ninfrastructure that it runs on.\n\nAlso, containers are often deployed on an infrastructure that has per-VM pricing.\nConsequently, as described earlier, you will likely incur the extra cost of overprovisioning\nVMs in order to handle spikes in load.\n\nInterestingly, the distinction between containers and VMs is likely to blur. As mentioned\nearlier, Boxfuse VMs are fast to build and start. The Clear Containers project aims to create\nlightweight VMs. There is also growing interest in unikernels. Docker, Inc acquired Unikernel\nSystems in early 2016.\n\nThere is also the newer and increasingly popular concept of server-less deployment,\nwhich is an approach that sidesteps the issue of having to choose between deploying\nservices in containers or VMs. Let's look at that next.\n\nMicroservices — From Design to Deployment 61 Ch. 6- Choosing a Microservices Depk Str",
            "status": "success"
        },
        {
            "page_number": 68,
            "text": "Serverless Deployment\n\nAWS Lambda is an example of serverless deployment technology. It supports Java,\nNodes, and Python services. To deploy a microservice, you package it as a ZIP file and\nupload it to AWS Lambda. You also supply metadata, which among other things specifies\nthe name of the function that is invoked to handle a request (a.k.a. an event). AWS Lambda\nautomatically runs enough instances of your microservice to handle requests. You are\nsimply billed for each request based on the time taken and the memory consumed.\nOf course, the devil is in the details, and you will see shortly that AWS Lambda has limitations.\nBut the notion that neither you as a developer, nor anyone in your organization, need worry\nabout any aspect of servers, virtual machines, or containers is incredibly appealing.\n\nA Lambda function is a stateless service. It typically handles requests by invoking AWS\nservices. For example, a Lambda function that is invoked when an image is uploaded to\nan S3 bucket could insert an item into a DynamoDB images table and publish a message\nto a Kinesis stream to trigger image processing. A Lambda function can also invoke\nthird-party web services.\n\nThere are four ways to invoke a Lambda function:\n\n¢ Directly, using a web service request\n\n* Automatically, in response to an event generated by an AWS service such as S3,\nDynamoDB, Kinesis, or Simple Email Service\n\n* Automatically, viaan AWS AP! Gateway to handle HTTP requests from clients of\nthe application\n\n¢ Periodically, according to a cron-like schedule\n\nAs you can see, AWS Lambda is a convenient way to deploy microservices. The request-\nbased pricing means that you only pay for the work that your services actually perform.\nAlso, because you are not responsible for the IT infrastructure, you can focus on\ndeveloping your application.\n\nThere are, however, some significant limitations. Lambda functions are not intended to\nbe used to deploy long-running services, such as a service that consumes messages\nfrom a third-party message broker. Requests must complete within 300 seconds.\nServices must be stateless, since in theory AWS Lambda might run a separate instance\nfor each request. They must be written in one of the supported languages. Services must\nalso start quickly; otherwise, they might be timed out and terminated.\n\nMicroservices — From Design to Deployment 62 Ch. 6 - Choosing a Microservices Depl Str",
            "status": "success"
        },
        {
            "page_number": 69,
            "text": "Summary\n\nDeploying a microservices application is challenging. You may have tens or even hundreds\nof services written in a variety of languages and frameworks. Each one is a mini-application\nwith its own specific deployment, resource, scaling, and monitoring requirements. There are\nseveral microservice deployment patterns, including Service Instance per Virtual Machine\nand Service Instance per Container. Another intriguing option for deploying microservices\nis AWS Lambda, a serverless approach. In the next and final chapter of this ebook, we will\nook at how to migrate a monolithic application to a microservices architecture.\n\nMicroservices in Action: Deploying Microservices Across\nVarying Hosts with NGINX\nby Floyd Smith\n\nNGINX has a lot of advantages for various types of deployment — whether for monolithic applica-\ntions, microservices apps, or hybrid apps (as described in the next chapter). With NGINX, you can\nabstract intelligence out of different deployment environments and into NGINX. There are many\napp capabilities that work differently if you use tools that are specific to different deployment\nenvironments, but that work the same way across all environments if you use NGINX.\n\nThis characteristic also opens up a second specific advantage for NGINX and NGINX Plus: the\nability to scale an app by running it in multiple deployment environments at the same time. Let's\nsay you have on-premise servers that you own and manage, but your app usage is growing and\nyou anticipate spikes beyond what those servers can handle. Instead of buying, provisioning, and\nkeeping additional servers warm “just in case\", if you've “gone NGINX\", you have a powerful alter-\nnative: scale into the cloud-—for instance, scale onto AWS. Thatis, handle traffic on your on-premise\nservers until capacity is reached, then spin up additional microservice instances in the cloud\nas needed.\n\nThis is just one example of the flexibility that a move to NGINX makes possible. Maintaining separate\ntesting and deployment environments, switching the infrastructure of your environments, and\nmanaging a portfolio of apps across all kinds of environments all become much more realistic\nand achievable.\n\nThe NGINX Microservices Reference Architecture is explicitly designed to support this kind o\nflexible deployment, with use of containers during development and deployment as an assumption.\nConsider a move to containers, if you're not there already, and to NGINX or NGINX Plus to ease\nyour move to microservices and to future-proof your apps, development and deployment flexibility,\nand personnel.\n\nMicroservices — From Design to Deployment 63 Ch.6 »sing a",
            "status": "success"
        },
        {
            "page_number": 70,
            "text": "Refactoring a\nMonolith into\nMicroservices\n\nThis is the seventh and final chapter in this ebook about building applications with\nmicroservices. Chapter 1 introduces the Microservice Architecture pattern and discusses\nhe benefits and drawbacks of using microservices. The subsequent chapters discuss\ndifferent aspects of the microservices architecture: using an API Gateway, inter-process\ncommunication, service discovery, event-driven data management, and deploying\nmicroservices. In this chapter, we look at strategies for migrating a monolithic application\n\nto microservices.\nhope that this ebook has given you a good understanding of the microservices architecture,\nits benefits and drawbacks, and when to use it. Perhaps the microservices architecture\n\nis a good fit for your organization.\n\nHowever, there is fairly good chance you are working on a large, complex monolithic\napplication. Your daily experience of developing and deploying your application is slow\nand painful. Microservices seem like a distant nirvana. Fortunately, there are strategies\nthat you can use to escape from the monolithic hell. In this article, | describe how to\nincrementally refactor a monolithic application into a set of microservices.\n\n64 Ch. 7 — Refactoring a Monolith into M\n\nMicroservices — From Design to Deployment",
            "status": "success"
        },
        {
            "page_number": 71,
            "text": "Overview of Refactoring to Microservices\n\nThe process of transforming a monolithic application into microservices is a form of\napplication modernization. That is something that developers have been doing for\nIt, there are some ideas that we can reuse when refactoring an\napplication into microservices.\n\ndecades. As aresu\n\nOne strategy not to use is the \"Big Bang\" rewrite. That is when you focus all of your\n\ndevelopment effor\n\nsonb\n\nuilding a new microservices-based application from scratch.\n\nAlthough it sounds appealing, it is extremely risky and will likely end in failure. As Martin\nFowler reportedly said, “the only thing a Big Bang rewrite guarantees is a Big Bang!\"\n\nInstead of a Big Ban\n\ng rewri\n\ne, you should incrementally refactor your monolithic application.\n\nYou gradually add new functionality, and create extensions of existing functionality, in the\nform of microservices — modifying your monolithic application ina complementary fashion,\n\nand running the microservi\n\nces and the modified monolith in tandem. Over time, the amount\n\nof functionality implemented by the monolithic application shrinks, until either it disappears\n\nentirely or it becomes just\nwhile driving down the hig\n\na Big Bang rewrite.\n\nMartin Fowler refers to this application\nmodernization strategy as the Strangler\nApplication. The name comes from the\nstrangler vine (a.k.a. stran\nis found in rainforests. A strangler vine\ngrows around a tree in order to reach\nthe sunlight above the fo\nSometimes, the tree dies, leaving a tree-\nshaped vine. Application modernization\nfollows the same pattern. We will\n\nbuild a new application consisting\n\nof microservices around the legacy\napplication, which will shrink and\n\nanother microservice. This strategy is akin to servicing your car\nhway at 70 mph — challenging, but far less risky than attempting\n\ngler fig) that\n\nperhaps, eventually, die.\n\nLet's look at different strategies\n\nfor doing this.\n\nest Canopy.\n\nMicroservices — From Design to Deployment\n\n65 Ch. 7 - Refac ya Monolith int",
            "status": "success"
        },
        {
            "page_number": 72,
            "text": "Strategy #1 —- Stop Digging\n\nThe Law of Holes says that whenever you are in a hole you should stop digging. This is\ngreat advice to follow when your monolithic application has become unmanageable. In\nother words, you should stop making the monolith bigger. This means that when you are\nimplementing new functionality you should not add more code to the monolith. Instead,\nthe big idea with this strategy is to put that new code in a standalone microservice.\n\nFigure 7-1 shows the system architecture after applying this approach.\n\nHTTP REQUEST\n\nREQUEST\nROUTER\n\nOld HTTP\nrequests\n\nNew HTTP\nrequests\n\nWEB\nAPI\n\nSERVICE\n\nFigure 7-1. Implementing new functionality as a separate service instead of adding a module to the monolith.\n\nMicroservices — From Design to Deployment 66 C — Refactoring a Monolith into M",
            "status": "success"
        },
        {
            "page_number": 73,
            "text": "As well as the new service and the legacy monolith, there are two other components.\nThe first is a request router, which handles incoming (HTTP) requests. It is similar to the\nAPI gateway described in Chapter 2. The router sends requests corresponding to new\nfunctionality to the new service. It routes legacy requests to the monolith.\n\nThe other component is the glue code, which integrates the service with the monolith.\nAservice rarely exists in isolation and often needs to access data owned by the monolith.\nThe glue code, which resides in either the monolith, the service, or both, is responsible\nfor the data integration. The service uses the glue code to read and write data owned by\nthe monolith.\n\nThere are three strategies that a service can use to access the monolith’s data:\n\n* Invoke a remote API provided by the monolith\n* Access the monolith's database directly\n* Maintain its own copy of the data, which is synchronized with the monolith’s database\n\nThe glue code is sometimes called an anti-corruption layer. That is because the glue code\nprevents the service, which has its own pristine domain model, from being polluted by\nconcepts from the legacy monolith’s domain model. The glue code translates between\nthe two different models. The term anti-corruption layer first appeared in the must-read\nbook Domain Driven Design by Eric Evans and was then refined in a white paper. Developing\nan anti-corruption layer can be a non-trivial undertaking. But it is essential to create one\nif you want to grow your way out of monolithic hell.\n\nmplementing new functionality as a lightweight service has a couple of benefits. It prevents\nthe monolith from becoming even more unmanageable. The service can be developed,\ndeployed, and scaled independently of the monolith. You experience the benefits of the\nmicroservice architecture for each new service that you create.\n\nHowever, this approach does nothing to address the problems with the monolith. To fix\nthose problems you need to break up the monolith. Let's look at strategies for doing that.\n\nStrategy #2 - Split Frontend and Backend\n\nA strategy that shrinks the monolithic application is to split the presentation layer from\nthe business logic and data access layers. A typical enterprise application consists of\nat least three different types of components:\n\n* Presentation layer - Components that handle HTTP requests and implement either a\n(REST) API or an HTML-based web UI. In an application that has a sophisticated user\ninterface, the presentation tier is often a substantial body of code.\n\n* Business logic layer - Components that are the core of the application and implement\nthe business rules.\n\n* Data-access layer - Components that access infrastructure components, such as\ndatabases and message brokers.\n\nMicroservices — From Design to Deployment 67 —Refe th int",
            "status": "success"
        },
        {
            "page_number": 74,
            "text": "There is usually a clean separation between the presentation logic on one side and the\nbusiness and data-access logic on the other. The business tier has a coarse-grained\nAPI consisting of one or more facades, which encapsulate business-logic components.\nThis API is a natural seam along which you can split the monolith into two smaller\napplications. One application contains the presentation layer. The other application\ncontains the business and data-access logic. After the split, the presentation logic\napplication makes remote calls to the business logic application.\n\nFigure 7-2 shows the architecture before and after the refactoring.\n\nBROWSER BROWSER\n\nwceeed owe pees wes\n. APPLICATION . APPLICATION\n\npatapase |... *\n‘ADAPTER\n\n«| patapase |... *\n‘ADAPTER\nysaL,\n\nFigure 7-2. Refactoring an existing app.\n\nSplitting a monolith in this way has two main benefits. It enables you to develop, deploy,\nand scale the two applications independently of one another. In particular, it allows the\npresentation-layer developers to iterate rapidly on the user interface and easily perform\nA\\B testing, for example. Another benefit of this approach is that it exposes a remote API\nthat can be called by the microservices that you develop.\n\nThis strategy, however, is only a partial solution. It is very likely that one or both of the\ntwo applications will be an unmanageable monolith. You need to use the third strategy\nto eliminate the remaining monolith or monoliths.\n\nMicroservices — From Design to Deployment 68 Ch. 7 — Refactoring a Monolith inte",
            "status": "success"
        },
        {
            "page_number": 75,
            "text": "Strategy #3 - Extract Services\n\nThe third refactoring stra\nmicroservices. Each time\nOnce you have convert\nEither it disappears enti\n\negy is to turn existing modules within the monolith into standalone\nyou extract a module and turn it into a service, the monolith shrinks.\ned enough modules, the monolith will cease to be a problem.\nely or it becomes small enough that it is just another service.\n\nPrioritizing Which Mod\n\nA large, complex monolithic application consists of tens or hundreds of modules, all of\nwhich are candidates for extraction. Figuring out which modules to convert first is often\nchallenging. A good approach is to start with a few modules that are easy to extract. This will\ngive you experience with microservices in general and the extraction process in particular.\nAfter that, you should extract those modules that will give you the greatest benefit.\n\nules to Convert into Services\n\nConverting a module into a service is typically time consuming. You want to rank modules\nby the benefit you will receive. It is usually beneficial to extract modules that change\nfrequently. Once you have converted a module into a service, you can develop and deploy\nit independently of the monolith, which will accelerate development.\n\nIt is also beneficial to extract modules that\ndifferent from those of the rest of the monolith. It is useful, for example, to turn a module\nthat has an in-memory database into a service, which can then be deployed on hosts,\nwhether bare metal servers, VMs, or cloud instances, with large amounts of memory.\nSimilarly, it can be worthwhile to extract modules that implement computationally\n\nhave resource requirements significantly\n\nexpensive algorithms, since the service cant\nBy turning modules with particular resource\n\nyour application much easier and less expensive to scale.\n\nWhen figuring out which modules to extract,\n\nhen be deployed on hosts with lots of CPUs.\nrequirements into services, you can make\n\nit is useful to look for existing coarse-grained\n\nboundaries (a.k.a seams). They make it easier and cheaper to turn modules into services.\nAn example of such a boundary is a module that only communicates with the rest of the\napplication via asynchronous messages. It can be relatively cheap and easy to turn that\nmodule into a microservice.\n\nHow to Extract a Module\n\nThe first step of extracting a module is to define a coarse-grained interface between the\nmodule and the monolith. It is mostly likely a bidirectional API, since the monolith will need\ndata owned by the service and vice versa. It is often challenging to implement such an\nAPI because of the tangled dependencies and fine-grained interaction patterns between\nthe module and the rest of the application. Business logic implemented using the Domain\nModel pattern is especially challenging to refactor because of numerous associations\nbetween domain model classes. You will often need to make significant code changes\nto break these dependencies. Figure 7-3 shows the refactoring.\n\n69\n\nMicroservices — From Design to Deployment",
            "status": "success"
        },
        {
            "page_number": 76,
            "text": "ed\n\nDATABASE DATABASE\nADAPTER ADAPTER\n\nDATABASE\nADAPTER\n\nmys@L\n\nFigure 7-3. A module from a monolith can become a microservice.\n\nMicroservices — From Design to Deployment 70 Ch. olith into M",
            "status": "success"
        },
        {
            "page_number": 77,
            "text": "Once you implement the coarse-grained interface, you then turn the module into a free-\nstanding service. To do that, you must write code to enable the monolith and the service to\ncommunicate through an API that uses an inter-process communication (IPC) mechanism.\nFigure 7-3 shows the architecture before, during, and after the refactoring.\n\nnthis example, Module Z is the candidate module to extract. Its components are used\nby Module X and it uses Module Y. The first refactoring step is to define a pair of coarse-\ngrained APIs. The first interface is an inbound interface that is used by Module X to invoke\nodule Z. The second is an outbound interface used by Module Z to invoke Module Y.\n\nThe second refactoring step turns the module into a standalone service. The inbound and\noutbound interfaces are implemented by code that uses an IPC mechanism. You will most\nikely need to build the service by combining Module Z with a Microservice Chassis\nramework that handles cross-cutting concerns such as service discovery.\n\nOnce you have extracted a module, you have yet another service that can be developed,\ndeployed, and scaled independently of the monolith and any other services. You can even\nrewrite the service from scratch; in this case, the API code that integrates the service\nwith the monolith becomes an anti-corruption layer that translates between the two\ndomain models. Each time you extract a service, you take another step in the direction\nof microservices. Over time, the monolith will shrink and you will have an increasing number\nof microservices.\n\nSummary\n\nThe process of migrating an existing application into microservices is a form of application\nmodernization. You should not move to microservices by rewriting your application\nfrom scratch. Instead, you should incrementally refactor your application into a set of\nmicroservices. There are three strategies you can use: implementing new functionality\nas microservices; splitting the presentation components from the business and data\naccess components; and converting existing modules in the monolith into services.\nOver time the number of microservices will grow, and the agility and velocity of your\ndevelopment team will increase.\n\nMicroservices — From Design to Deployment 71 —Refe thint",
            "status": "success"
        },
        {
            "page_number": 78,
            "text": "Microservices in Action: Taming a Monolith with NGINX\nby Floyd Smith\n\nAs this chapter describes, converting a monolith to microservices is likely to be a slow and chal-\nlenging process, yet one with many benefits. With NGINX, you can begin to get some of the benefits\nof microservices before you actually begin the conversion process.\n\nYou can buy a lot of time for the move to microservices by “dropping NGINX in front of” your existing\nmonolithic application. Here's a brief description of the benefits as they relate to microservices:\n\n¢ Better support for microservices — As mentioned in the sidebar for Chapter 5, NGINX, and\nNGINX Plus in particular, have capabilities that help enable the development of microservices-\nbased apps. As you begin to re-design your monolithic application, your microservices will\nperform better and be easier to manage due to the capabilities in NGINX.\n\n* Functional abstraction across environments — Moving capabilities onto NGINX as a reverse\nproxy server reduces the number of things that will vary when you deploy across new environ-\nments, from servers you manage to various flavors of public, private, and hybrid clouds. This\ncomplements and extends the flexibility inherent to microservices.\n\n* Availability of the NGINX Microservices Reference Architecture — As you move to NGINX,\nyou can borrow from the NGINX Microservices Reference Architecture, both to define the ultimate\nstructure of your app after the move to microservices, and to use parts of the MRA as needed\nfor each new microservice you create.\n\nTo sum up, implementing NGINX as a first step in your transition takes the pressure off your\nmonolithic application, makes it much easier to attain all of the benefits of microservices, and\ngives you models for use in making the transition. You can learn more about the MRA and get a\nfree trial of NGINX Plus today.\n\nMicroservices — From Design to Deployment 72",
            "status": "success"
        },
        {
            "page_number": 79,
            "text": "Resources for Microservices and NGINX\nby Floyd Smith\n\nThe NGINX website is already a valued resource for people seeking to learn about\nmicroservices and implement them in their organizations. From introductory descriptions,\nsuch as the first chapter of this ebook, to advanced resources such as the Fabric Model\nof the NGINX Microsoft Reference Architecture, there's a graduate seminar-level course\nin microservices available at https://www.nginx.com.\n\nHere are a few tips and tricks, and a few key resources, for getting started on your\njourney with NGINX and microservices:\n\n° Site search and web search. The best way to search the NGINX website for\nmicroservices material is to use site-specific search in Google:\n\n°\n\nsite:nginx.com topic to search the NGINX website.\n\nsite:nginx.com/blog topic to search the NGINX blog. All blog posts are tagged,\nso once you find a topic you want to follow up on, just click the tag to see all\nrelevant posts. Authors are linked to all their articles as well.\n\nSearch for topic nginx to find content relevant to both NGINX and your topic of\nchoice on the Web as a whole — there's a lot of great stuff out there. DigitalOcean\nmay be the best external place to start.\n\n°\n\n°\n\nGeneral NGINX resources. Here are links to different types of content on the NGINX site:\n\n© Blog posts. Once you find a post on microservices, click the microservices tag to see all\nsuch posts.\n\no Webinars. Click the Microservices filter to see microservices-relevant webinars.\n\no White papers, reports, and ebooks. Use site search on this part of the site,\nas described above, to find resources relating specifically to microservices and\nother topics of your choice.\n\no NGINX YouTube channel. NGINX has dozens of videos, including all the presentations\nfrom several years of our annual conference. Many of these videos have been\nconverted into blog posts if you prefer reading to watching; search for the name of\nthe speaker in the NGINX blog.\n\nMicroservices — From Design to Deployment 73",
            "status": "success"
        },
        {
            "page_number": 80,
            "text": "° Specific resources. Microservices is the single most popular, and best-covered, topic\non the NGINX website. Here are a few “best of the best\" resources to get you started.\n\no This ebook as a blog post series. Look in the NGINX blog to find the Chris Richardson\nblog posts that were (lightly) adapted to form the seven chapters of this ebook.\n\n© Building Microservices ebook. A free download of an O'Reilly animal book on\n\nmicroservices. Need we say more?\n\no Microservices at Netflix. Netflix is a leader in implementing microservices, moving\n\no the cloud, and making their efforts available as open source — all based on NGINX,\n\nof course.\n\no Why NGINX for Containers and Microservices? The inimitable Owen Garrett on a\n\nopic dear to our hearts.\n\n© Implementing Microservices. A fresh take on the topic of this ebook, emphasizing\n\nhe four-tier architecture.\n\nIntroducing the NGINX Microservices Reference Architecture. Professional services\n\nmaven Chris Stetson introduces the MRA.\n\n°\n\nMicroservices — From Design to Deployment 74 Resources for Microservice",
            "status": "success"
        }
    ],
    "languages": [
        "eng"
    ],
    "page_count": 80,
    "status": "success"
}